# مستندات ماژول Data در Smart Whale AI

## 📌 مقدمه
ماژول Data قلب تپنده سیستم Smart Whale است که وظیفه جمع‌آوری، پردازش و مدیریت انواع داده‌ها را بر عهده دارد. این ماژول با تمرکز بر اصل "کمترین پردازش و بهترین نتیجه" طراحی شده است و با استفاده از مکانیزم‌های پیشرفته جریان داده و پردازش بهینه، بالاترین کارایی را با حداقل مصرف منابع ارائه می‌دهد.

## 🎯 محدوده مسئولیت‌ها و اهداف

### وظایف اصلی
ماژول Data با دقت فراوان وظایف خود را محدود و مشخص کرده است تا از پردازش‌های اضافی جلوگیری شود. این وظایف عبارتند از:

- جمع‌آوری هوشمند داده از منابع مختلف با تمرکز بر کاهش سربار ارتباطی
- نرمال‌سازی اولیه داده‌ها تنها در سطح ساختاری برای حداقل پردازش
- تبدیل به فرمت استاندارد با استفاده از مکانیزم‌های بهینه تبدیل
- مدیریت هوشمند روابط بین داده‌ها با حداقل افزونگی
- ذخیره‌سازی بهینه با استفاده از تکنیک‌های فشرده‌سازی و کش‌گذاری
- انتشار کارآمد داده‌ها روی Kafka با بهره‌گیری از پردازش جریانی

### خارج از محدوده مسئولیت
برای اطمینان از عدم تکرار پردازش و حفظ اصل تفکیک وظایف، موارد زیر خارج از محدوده این ماژول قرار دارند:

- تمیزسازی تخصصی داده‌ها (این وظیفه به عهده هر مدل است)
- دسته‌بندی محتوایی داده‌ها
- تشخیص کیفیت محتوا
- پردازش معنایی داده‌ها
- تحلیل احساسات
- استخراج موجودیت‌ها

## 🏗️ ساختار ماژول

ساختار ماژول Data به گونه‌ای طراحی شده که حداکثر کارایی را با حداقل وابستگی بین اجزا فراهم کند:

# ساختار کامل ماژول Data

```
data/
├── stream/                         # مدیریت جریان داده
│   ├── processor/
│   │   ├── stream_processor.py     # پردازش جریانی بهینه
│   │   ├── batch_processor.py      # پردازش دسته‌ای هوشمند
│   │   └── flow_controller.py      # کنترل جریان داده
│   │
│   ├── buffer/
│   │   ├── smart_buffer.py         # بافر با مدیریت هوشمند حافظه
│   │   ├── memory_optimizer.py     # بهینه‌سازی مصرف حافظه
│   │   └── overflow_handler.py     # مدیریت سرریز داده
│   │
│   └── optimizer/
│       ├── throughput_optimizer.py  # بهینه‌سازی توان عملیاتی
│       └── resource_balancer.py     # متعادل‌سازی مصرف منابع
│
├── collectors/                      # جمع‌آورنده‌های داده
│   ├── base/
│   │   ├── collector.py            # کلاس پایه با عملکرد بهینه
│   │   ├── source_manager.py       # مدیریت منابع داده
│   │   └── error_handler.py        # مدیریت خطاها
│   │
│   ├── text/                       # جمع‌آوری داده‌های متنی
│   │   ├── web_collector/
│   │   │   ├── general_crawler.py   # خزش عمومی وب
│   │   │   ├── targeted_crawler.py  # خزش هدفمند
│   │   │   └── api_collector.py     # جمع‌آوری از APIها
│   │   │
│   │   ├── social/
│   │   │   ├── twitter_collector.py  # جمع‌آوری از توییتر
│   │   │   ├── telegram_collector.py # جمع‌آوری از تلگرام
│   │   │   └── linkedin_collector.py # جمع‌آوری از لینکدین
│   │   │
│   │   ├── document/
│   │   │   ├── pdf_collector.py     # جمع‌آوری از PDF
│   │   │   ├── word_collector.py    # جمع‌آوری از Word
│   │   │   └── text_collector.py    # جمع‌آوری از متن ساده
│   │   │
│   │   └── specialized/
│   │       ├── wiki_collector.py    # جمع‌آوری از ویکی‌پدیا
│   │       ├── news_collector.py    # جمع‌آوری از اخبار
│   │       └── blog_collector.py    # جمع‌آوری از وبلاگ‌ها
│   │
│   ├── media/                      # جمع‌آوری داده‌های چندرسانه‌ای
│   │   ├── image/
│   │   │   ├── web_image.py        # تصاویر از وب
│   │   │   ├── storage_image.py    # تصاویر از مخزن
│   │   │   └── social_image.py     # تصاویر از شبکه‌های اجتماعی
│   │   │
│   │   ├── video/
│   │   │   ├── youtube.py          # ویدیو از یوتیوب
│   │   │   ├── aparat.py          # ویدیو از آپارات
│   │   │   └── social_video.py    # ویدیو از شبکه‌های اجتماعی
│   │   │
│   │   └── audio/
│   │       ├── podcast.py          # جمع‌آوری پادکست
│   │       ├── music.py            # جمع‌آوری موسیقی
│   │       └── voice.py            # جمع‌آوری صدا
│   │
│   └── composite/                  # جمع‌آوری داده‌های ترکیبی
│       ├── hybrid_collector.py     # جمع‌آورنده ترکیبی
│       └── relation_manager.py     # مدیریت روابط بین داده‌ها
│
├── intelligence/                    # هوش سیستم
│   ├── optimizer/
│   │   ├── stream_optimizer.py     # بهینه‌سازی جریان
│   │   ├── chunk_analyzer.py       # تحلیل اندازه چانک‌ها
│   │   └── pattern_detector.py     # تشخیص الگوها
│   │
│   ├── scheduler/
│   │   ├── task_scheduler.py       # زمانبندی وظایف
│   │   ├── priority_manager.py     # مدیریت اولویت‌ها
│   │   └── workload_balancer.py    # متعادل‌سازی بار کاری
│   │
│   └── analyzer/
│       ├── performance_analyzer.py  # تحلیل کارایی
│       ├── bottleneck_detector.py   # تشخیص گلوگاه‌ها
│       └── efficiency_monitor.py    # پایش کارایی
│
├── storage/                         # ذخیره‌سازی داده
│   ├── persistent/
│   │   ├── clickhouse_manager.py   # مدیریت ClickHouse
│   │   ├── minio_manager.py        # مدیریت MinIO
│   │   └── elastic_manager.py      # مدیریت Elasticsearch
│   │
│   ├── cache/
│   │   ├── redis_manager.py        # مدیریت Redis
│   │   ├── memory_cache.py         # کش حافظه
│   │   └── distributed_cache.py    # کش توزیع‌شده
│   │
│   └── archive/
│       ├── compression_manager.py   # مدیریت فشرده‌سازی
│       ├── backup_manager.py        # مدیریت پشتیبان
│       └── cleanup_manager.py       # مدیریت پاکسازی
│
├── pipeline/                        # خط لوله پردازش
│   ├── stages/
│   │   ├── collector_stage.py      # مرحله جمع‌آوری
│   │   ├── processor_stage.py      # مرحله پردازش
│   │   └── publisher_stage.py      # مرحله انتشار
│   │
│   ├── orchestrator/
│   │   ├── flow_manager.py         # مدیریت جریان
│   │   ├── dependency_manager.py   # مدیریت وابستگی‌ها
│   │   └── error_handler.py        # مدیریت خطاها
│   │
│   └── optimizer/
│       ├── pipeline_optimizer.py    # بهینه‌ساز خط لوله
│       ├── stage_optimizer.py       # بهینه‌ساز مراحل
│       └── transition_optimizer.py  # بهینه‌ساز انتقال‌ها
│
└── monitoring/                      # پایش و نظارت
    ├── metrics/
    │   ├── collector.py            # جمع‌آوری متریک‌ها
    │   ├── aggregator.py           # تجمیع متریک‌ها
    │   └── exporter.py             # صادرکننده متریک‌ها
    │
    ├── alerts/
    │   ├── detector.py             # تشخیص هشدارها
    │   ├── notifier.py             # اطلاع‌رسانی
    │   └── handler.py              # مدیریت هشدارها
    │
    └── visualization/
        ├── dashboard_generator.py   # تولید داشبورد
        ├── report_generator.py      # تولید گزارش
        └── trend_visualizer.py      # نمایش روندها
```

## 📦 شرح تفصیلی بخش‌ها

### 1️⃣ مدیریت جریان داده (`stream/`)

مدیریت جریان داده یکی از کلیدی‌ترین بخش‌های این ماژول است که با هدف دستیابی به حداکثر کارایی با حداقل مصرف منابع طراحی شده است.

#### پردازشگر جریان (`processor/`)
این بخش مسئول پردازش بهینه جریان‌های داده است:

- **Stream Processor**: پردازشگر اصلی جریان
  - متدهای اصلی:
    - `process_stream`: پردازش جریانی با مدیریت هوشمند حافظه
    - `optimize_throughput`: بهینه‌سازی پویای توان عملیاتی
    - `handle_backpressure`: مدیریت فشار برگشتی و کنترل جریان
  - مکانیزم‌ها:
    - استفاده از الگوریتم‌های پردازش جریانی با حداقل تأخیر
    - تخصیص پویای منابع بر اساس حجم داده
    - مدیریت هوشمند بافر برای کاهش مصرف حافظه

[ادامه از بخش قبل...]

- **Batch Processor**: پردازشگر دسته‌ای
  - متدهای اصلی:
    - `batch_streams`: ترکیب هوشمند جریان‌های مشابه برای کاهش پردازش
    - `dynamic_batch_sizing`: تعیین پویای اندازه دسته بر اساس شرایط سیستم
    - `priority_batching`: دسته‌بندی با در نظر گرفتن اولویت‌ها
  - مکانیزم‌ها:
    - الگوریتم‌های پیشرفته تشخیص جریان‌های مشابه
    - تنظیم خودکار پارامترها بر اساس عملکرد سیستم
    - مدیریت حافظه با استفاده از تکنیک‌های حذف تکرار

- **Flow Controller**: کنترل‌کننده جریان
  - متدهای اصلی:
    - `control_flow_rate`: تنظیم نرخ جریان داده
    - `distribute_load`: توزیع بار بین پردازشگرها
    - `manage_congestion`: مدیریت ازدحام و جلوگیری از سرریز
  - مکانیزم‌ها:
    - الگوریتم‌های پیشرفته کنترل جریان
    - سیستم توزیع هوشمند بار
    - مکانیزم‌های پیشگیری از ازدحام

#### مدیریت بافر (`buffer/`)
این بخش با هدف مدیریت بهینه حافظه و جلوگیری از اتلاف منابع طراحی شده است:

- **Smart Buffer**: بافر هوشمند
  - متدهای اصلی:
    - `adaptive_buffering`: بافرینگ تطبیقی بر اساس شرایط
    - `memory_efficient_store`: ذخیره‌سازی با حداقل مصرف حافظه
    - `predictive_cleanup`: پاکسازی پیش‌بینانه بافر
  - مکانیزم‌ها~~:
    -~~ الگوریتم‌های پیش‌بینی نیاز به بافر
    - سیستم مدیریت چرخه حیات داده
    - بهینه‌سازی خودکار اندازه بافر

- **Memory Optimizer**: بهینه‌ساز حافظه
  - متدهای اصلی:
    - `optimize_allocation`: تخصیص بهینه حافظه
    - `detect_leaks`: تشخیص و رفع نشت حافظه
    - `compact_storage`: فشرده‌سازی هوشمند داده‌ها
  - مکانیزم‌ها:
    - الگوریتم‌های پیشرفته مدیریت حافظه
    - سیستم تشخیص خودکار الگوهای مصرف
    - مکانیزم‌های بازیافت هوشمند حافظه

### 2️⃣ جمع‌آوری داده (`collectors/`)

سیستم جمع‌آوری داده با تمرکز بر کارایی و بهینگی طراحی شده است:

#### کلاس‌های پایه (`base/`)
- **Base Collector**: جمع‌آورنده پایه
  - متدهای اصلی:
    - `efficient_collect`: جمع‌آوری بهینه داده
    - `validate_source`: اعتبارسنجی سریع منابع
    - `optimize_connection`: بهینه‌سازی اتصال به منبع
  - مکانیزم‌ها:
    - سیستم مدیریت اتصالات با حداقل سربار
    - پروتکل‌های بهینه انتقال داده
    - مکانیزم‌های کش‌گذاری هوشمند

#### جمع‌آورنده‌های تخصصی (`specialized/`)
- **Text Collector**: جمع‌آورنده متن
  - متدهای اصلی:
    - `extract_text`: استخراج بهینه متن
    - `detect_encoding`: تشخیص هوشمند کدگذاری
    - `normalize_text`: نرمال‌سازی سریع متن
  - مکانیزم‌ها:
    - الگوریتم‌های پیشرفته پردازش متن
    - سیستم تشخیص خودکار ساختار
    - مکانیزم‌های حذف محتوای تکراری

### 3️⃣ هوش سیستم (`intelligence/`)

بخش هوشمند سیستم که وظیفه بهینه‌سازی و تصمیم‌گیری را بر عهده دارد:

#### بهینه‌ساز (`optimizer/`)
- **Stream Optimizer**: بهینه‌ساز جریان
  - متدهای اصلی:
    - `analyze_patterns`: تحلیل الگوهای جریان داده
    - `optimize_routing`: مسیریابی بهینه جریان‌ها
    - `tune_parameters`: تنظیم خودکار پارامترها
  - مکانیزم‌ها:
    - الگوریتم‌های یادگیری الگوها
    - سیستم تصمیم‌گیری پویا
    - مکانیزم‌های خودتنظیمی

[ادامه از بخش قبل...]

#### زمانبند هوشمند (`scheduler/`)
- **Task Scheduler**: زمانبند وظایف
  - متدهای اصلی:
    - `schedule_tasks`: زمانبندی هوشمند با حداقل تداخل
    - `balance_workload`: توزیع متوازن بار کاری
    - `adapt_scheduling`: تطبیق برنامه با شرایط سیستم
  - مکانیزم‌ها:
    - الگوریتم‌های پیشرفته زمانبندی با پیش‌بینی بار
    - سیستم تشخیص تداخل‌های احتمالی
    - مکانیزم‌های تعدیل خودکار برنامه

#### تحلیلگر سیستم (`analyzer/`)
- **Performance Analyzer**: تحلیلگر کارایی
  - متدهای اصلی:
    - `analyze_metrics`: تحلیل متریک‌های عملکردی
    - `identify_bottlenecks`: شناسایی گلوگاه‌ها
    - `suggest_optimizations`: پیشنهاد بهینه‌سازی‌ها
  - مکانیزم‌ها:
    - الگوریتم‌های پیشرفته تحلیل عملکرد
    - سیستم پیش‌بینی مشکلات احتمالی
    - مکانیزم‌های بهبود خودکار عملکرد

## 🔄 مکانیزم‌های بهینه‌سازی کلیدی

### 1️⃣ مدیریت جریان داده
سیستم از چندین مکانیزم پیشرفته برای بهینه‌سازی جریان داده استفاده می‌کند:

- **پردازش جریانی تطبیقی**
  - تنظیم خودکار اندازه بافر بر اساس حجم داده
  - مدیریت هوشمند backpressure
  - بهینه‌سازی مسیر جریان داده

- **دسته‌بندی هوشمند**
  - ترکیب خودکار جریان‌های مشابه
  - تعیین پویای اندازه دسته‌ها
  - اولویت‌بندی هوشمند پردازش

### 2️⃣ مدیریت منابع
سیستم به طور مداوم منابع را پایش و بهینه می‌کند:

- **مدیریت حافظه پویا**
  - تخصیص هوشمند حافظه به جریان‌های داده
  - پاکسازی خودکار داده‌های غیرضروری
  - مدیریت سطوح مختلف کش

- **بهینه‌سازی CPU**
  - توزیع هوشمند بار پردازشی
  - اولویت‌بندی عملیات‌های حیاتی
  - مدیریت پویای thread pool

## 📊 سیستم مانیتورینگ و گزارش‌گیری

### 1️⃣ متریک‌های کلیدی
سیستم به طور مداوم این متریک‌ها را پایش می‌کند:

- نرخ پردازش داده
- میزان استفاده از منابع
- زمان پاسخ‌دهی
- نرخ خطا
- وضعیت بافرها

### 2️⃣ هشداردهی هوشمند
سیستم قادر به تشخیص و هشداردهی در موارد زیر است:

- افزایش غیرعادی مصرف منابع
- کاهش کارایی پردازش
- مشکلات اتصال به منابع داده
- سرریز بافرها

## 🛡️ مکانیزم‌های تضمین کیفیت

### 1️⃣ تضمین صحت داده
سیستم از چندین لایه کنترلی استفاده می‌کند:

- **اعتبارسنجی منبع**
  - بررسی اعتبار منابع داده
  - تشخیص داده‌های نامعتبر
  - کنترل کیفیت جریان داده

- **یکپارچگی داده**
  - حفظ توالی داده‌ها
  - تشخیص و رفع ناسازگاری‌ها
  - مدیریت نسخه‌های داده

### 2️⃣ مدیریت خطا
سیستم دارای مکانیزم‌های پیشرفته مدیریت خطا است:

- **تشخیص خطا**
  - شناسایی سریع خطاها
  - تحلیل ریشه‌ای مشکلات
  - پیش‌بینی خطاهای احتمالی

- **بازیابی خودکار**
  - مکانیزم‌های retry هوشمند
  - بازگشت به حالت پایدار
  - جایگزینی خودکار منابع معیوب

## 🔄 استراتژی‌های توسعه‌پذیری

برای اطمینان از توسعه‌پذیری سیستم، این مکانیزم‌ها پیاده‌سازی شده‌اند:

### 1️⃣ مقیاس‌پذیری افقی
- توزیع خودکار بار بین نمونه‌ها
- همگام‌سازی وضعیت بین نودها
- مدیریت پویای کلاستر

### 2️⃣ انعطاف‌پذیری
- معماری ماژولار و قابل گسترش
- رابط‌های استاندارد برای افزودن قابلیت‌های جدید
- پیکربندی پویا و قابل تنظیم

# راهنمای استفاده از ماژول Data

## 📌 مقدمه به نحوه استفاده
ماژول Data خدمات خود را از طریق یک رابط منسجم و یکپارچه ارائه می‌دهد. این راهنما نحوه استفاده از این خدمات را با تمرکز بر سناریوهای مختلف و بهترین شیوه‌های استفاده توضیح می‌دهد.

## 🔄 مدل تعامل با ماژول

### درخواست‌های همزمان
برای بهره‌برداری از قابلیت‌های همزمانی ماژول، درخواست‌ها باید به صورت async/await ارسال شوند:

```python
async def collect_data(source_config: SourceConfig) -> DataResult:
    collector = DataCollector()
    async with collector.session() as session:
        result = await session.collect(source_config)
        return result
```

برای استفاده بهینه از این قابلیت، توصیه می‌شود:
- همیشه از context manager برای مدیریت session استفاده کنید
- از batch processing برای درخواست‌های مشابه استفاده کنید
- تنظیمات timeout مناسب را اعمال کنید

### مدیریت جریان داده
برای کار با جریان‌های داده، از StreamProcessor استفاده کنید:

```python
async def process_data_stream(source: DataSource) -> AsyncIterator[DataChunk]:
    processor = StreamProcessor(chunk_size=1024)
    async for chunk in processor.process(source):
        # انجام عملیات روی هر چانک
        yield chunk
```

نکات کلیدی در استفاده از جریان داده:
- از مکانیزم backpressure برای کنترل سرعت استفاده کنید
- اندازه چانک را بر اساس نیاز خود تنظیم کنید
- از مکانیزم‌های خودکار بازیابی بهره ببرید

## 🎯 سناریوهای استفاده رایج

### سناریو 1: جمع‌آوری داده از منابع وب
این سناریو نحوه جمع‌آوری بهینه داده از منابع وب را نشان می‌دهد:

```python
async def collect_web_data(urls: List[str]) -> DataCollection:
    collector = WebCollector(
        concurrent_limit=10,
        retry_policy=RetryPolicy(max_retries=3)
    )

    results = await collector.collect_batch(
        urls,
        batch_size=100,
        prioritize=True
    )
    
    return results
```

مزایای این رویکرد:
- مدیریت خودکار محدودیت درخواست‌های همزمان
- پردازش دسته‌ای برای کارایی بیشتر
- اولویت‌بندی هوشمند درخواست‌ها

### سناریو 2: پردازش داده‌های حجیم
برای پردازش داده‌های حجیم از مکانیزم جریانی استفاده کنید:

```python
async def process_large_dataset(dataset: Dataset) -> ProcessedResult:
    processor = StreamProcessor(
        memory_limit='2GB',
        optimization_level='high'
    )

    async with processor.batch_context() as batch:
        result = await batch.process_stream(
            dataset,
            chunk_processor=your_processor_func
        )
    
    return result
```

مزایای این رویکرد:
- مصرف بهینه حافظه
- پردازش خودکار دسته‌ای
- مدیریت هوشمند منابع

## 🛠️ تنظیمات و بهینه‌سازی

### تنظیمات کارایی
برای دستیابی به بهترین عملکرد، این پارامترها را تنظیم کنید:

```python
config = DataConfig(
    batch_size=1000,              # اندازه دسته‌های پردازشی
    buffer_size='100MB',          # اندازه بافر
    concurrency_limit=20,         # محدودیت همزمانی
    optimization_level='high',     # سطح بهینه‌سازی
    cache_policy=CachePolicy(     # سیاست کش‌گذاری
        ttl='1h',
        max_size='1GB'
    )
)
```

### مدیریت منابع
برای مدیریت بهینه منابع، از ResourceManager استفاده کنید:

```python
async with ResourceManager(config) as resources:
    await resources.allocate('memory', '2GB')
    await resources.set_priority('high')
    
    # انجام عملیات با منابع تخصیص یافته
    result = await process_data(data)
```

## 🔍 مانیتورینگ و عیب‌یابی

### جمع‌آوری متریک‌ها
برای پایش عملکرد سیستم، از MetricsCollector استفاده کنید:

```python
metrics = MetricsCollector(
    interval='1m',           # فاصله جمع‌آوری متریک‌ها
    aggregation='avg',       # نوع تجمیع
    export_format='json'     # فرمت خروجی
)

async with metrics.collect():
    # انجام عملیات
    await process_data(data)
```

### ثبت و بررسی خطاها
برای مدیریت و بررسی خطاها:

```python
error_handler = ErrorHandler(
    log_level='debug',
    retry_enabled=True,
    alert_threshold=5
)

try:
    await process_data(data)
except DataError as e:
    await error_handler.handle(e)
```

## ⚡️ نکات کلیدی عملکرد

برای دستیابی به بهترین عملکرد، این موارد را رعایت کنید:

1. **مدیریت اتصال‌ها**:
   - همیشه از connection pooling استفاده کنید
   - اتصال‌ها را به موقع آزاد کنید
   - از مکانیزم‌های retry مناسب استفاده کنید

2. **بهینه‌سازی حافظه**:
   - از streaming برای داده‌های حجیم استفاده کنید
   - بافرها را به موقع پاک کنید
   - از مکانیزم‌های کش‌گذاری هوشمند استفاده کنید

3. **مدیریت خطا**:
   - همیشه از try-except استفاده کنید
   - خطاها را به درستی لاگ کنید
   - از مکانیزم‌های بازیابی خودکار استفاده کنید

## 🚀 بهترین شیوه‌های استفاده

برای استفاده بهینه از ماژول Data، این اصول را رعایت کنید:

1. **پردازش دسته‌ای**:
   - درخواست‌های مشابه را گروه‌بندی کنید
   - از batch processing استفاده کنید
   - اندازه دسته‌ها را بهینه کنید

2. **مدیریت منابع**:
   - منابع را به موقع آزاد کنید
   - از context managers استفاده کنید
   - محدودیت‌های منابع را رعایت کنید

3. **کش‌گذاری**:
   - از استراتژی‌های کش مناسب استفاده کنید
   - TTL را به درستی تنظیم کنید
   - کش را به موقع پاک کنید
# مستندات سرویس‌های ماژول Data در Smart Whale AI

## 📌 مقدمه
ماژول Data بخشی حیاتی از Smart Whale AI است که مسئول جمع‌آوری داده از منابع مختلف و ارسال نتایج به مدل‌ها می‌باشد. در این بخش، سرویس‌هایی پیاده‌سازی شده‌اند که وظیفه مدیریت جمع‌آورنده‌ها، راه‌اندازی فرآیند جمع‌آوری داده، مدیریت پیام‌رسانی مرتبط با داده و ارسال نتایج جمع‌آوری شده به مدل‌ها را بر عهده دارند. این سرویس‌ها از امکانات ارتباطی ماژول Messaging، KafkaService و TopicManager استفاده می‌کنند تا تضمین کنند که عملیات جمع‌آوری و انتقال داده‌ها به صورت استاندارد، امن و با کارایی بالا انجام شود.

---

## 🏗️ ساختار ماژول

دایرکتوری **ai/data/services/** شامل پنج فایل اصلی است که ساختار آن به صورت زیر می‌باشد:

```
ai/data/services/
├── collector_manager.py        # مدیریت جمع‌آورنده‌های داده و بهینه‌سازی استفاده از آن‌ها
├── data_collector_service.py   # سرویس اصلی جمع‌آوری داده بر اساس درخواست‌های دریافتی
├── messaging_service.py        # سرویس پیام‌رسانی برای ماژول Data
├── result_service.py           # سرویس ارسال نتایج جمع‌آوری داده به مدل‌ها
└── __init__.py                 # صادرسازی سرویس‌های Data برای استفاده در سایر بخش‌ها
```

---

## 📦 شرح تفصیلی سرویس‌ها

### 1️⃣ مدیریت جمع‌آورنده‌ها (CollectorManager) - فایل `collector_manager.py`
این سرویس مسئول مدیریت و بهینه‌سازی استفاده از جمع‌آورنده‌های مختلف داده است. از وظایف آن می‌توان به ثبت و نگهداری کلاس‌های جمع‌آورنده، ایجاد نمونه‌های فعال، ثبت وضعیت آن‌ها و آزادسازی منابع پس از پایان کار اشاره کرد.

#### ویژگی‌ها و عملکردها:
- **ثبت کلاس‌های جمع‌آورنده:**  
  متد `_register_collector_classes()`، کلاس‌های جمع‌آورنده مربوط به منابع مختلف مانند **WikiCollector** برای منابع ویکی و **GeneralWebCrawler** برای جمع‌آوری داده‌های وب را ثبت می‌کند.
  
- **دریافت نمونه جمع‌آورنده:**  
  متد `get_collector(collector_type, source_id, **params)` یک نمونه از جمع‌آورنده را ایجاد می‌کند یا در صورت وجود نمونه فعال، همان نمونه را بازمی‌گرداند.
  
- **آزادسازی جمع‌آورنده:**  
  متد `release_collector(collector_type, source_id)` جمع‌آورنده فعال را متوقف کرده و منابع مربوط به آن را آزاد می‌کند.
  
- **جمع‌آوری داده:**  
  متد `collect_data(data_type, data_source, parameters)`:
  - نوع داده و منبع داده را به رشته تبدیل می‌کند.
  - با استفاده از متد `_get_collector_type`، نوع مناسب جمع‌آورنده را بر اساس منبع و نوع داده تشخیص می‌دهد.
  - با استفاده از متد `_prepare_collector_params`، پارامترهای لازم برای ایجاد جمع‌آورنده را آماده می‌کند.
  - نمونه جمع‌آورنده را ایجاد، داده‌ها را جمع‌آوری کرده و پس از اتمام کار، آن را آزاد می‌کند.
  
- **متدهای کمکی:**
  - `_get_collector_type(data_source, data_type)`: تعیین نوع جمع‌آورنده مناسب بر اساس نگاشت از منبع داده به نوع جمع‌آورنده.
  - `_prepare_collector_params(collector_type, parameters)`: آماده‌سازی پارامترهای خاص برای هر نوع جمع‌آورنده (مانند تنظیم زبان و عنوان برای ویکی).
  - `_get_timestamp()`: تولید رشته زمان فعلی جهت ثبت زمان ایجاد یا آزادسازی نمونه‌ها.

- **نمونه Singleton:**  
  - `collector_manager` به عنوان یک شیء singleton از کلاس CollectorManager در سراسر سیستم قابل استفاده است.

---

### 2️⃣ سرویس جمع‌آوری داده (DataCollectorService) - فایل `data_collector_service.py`
این سرویس اصلی جمع‌آوری داده است که وظیفه دریافت درخواست‌های جمع‌آوری داده از طریق Kafka، هدایت آن‌ها به جمع‌آورنده‌های مناسب و ارسال نتایج جمع‌آوری‌شده به عنوان پاسخ را بر عهده دارد.

#### ویژگی‌ها و عملکردها:
- **ثبت جمع‌آورنده‌های پشتیبانی‌شده:**  
  متد `_register_collectors()`، جمع‌آورنده‌های پشتیبانی شده برای داده‌های متنی (مثلاً ویکی و وب) را ثبت می‌کند. این ثبت به صورت دیکشنری است که کلید آن‌ها نوع منبع (DataSource) و زیر کلید آن نوع داده (DataType) می‌باشد.
  
- **آماده‌سازی اولیه:**  
  متد `initialize()`، اتصال به Kafka را برقرار کرده و اطمینان حاصل می‌کند که موضوع درخواست‌های داده (DATA_REQUESTS_TOPIC) در Kafka وجود دارد.
  
- **اجرا و گوش دادن به درخواست‌ها:**  
  متد `run()`، سرویس جمع‌آوری داده را اجرا کرده و از طریق Kafka به درخواست‌های دریافتی گوش می‌دهد. در این حالت، با استفاده از group_id مشخص، پیام‌های دریافتی پردازش می‌شوند.
  
- **پردازش درخواست‌ها:**  
  متد `process_request(request_data)`، داده‌های دریافتی را به یک نمونه DataRequest تبدیل کرده و بر اساس نوع عملیات (مثلاً FETCH_DATA) آن‌ها را به متد `collect_data()` هدایت می‌کند.
  
- **جمع‌آوری داده بر اساس درخواست:**  
  متد `collect_data(request, response_topic)`:
  - اطلاعات درخواست مانند model_id، نوع داده، منبع داده و پارامترها استخراج می‌شوند.
  - در صورت نیاز، مقادیر رشته‌ای به Enum تبدیل می‌شوند یا با تشخیص خودکار منبع تنظیم می‌گردند.
  - در صورت عدم وجود جمع‌آورنده مناسب، یک پاسخ خطا ارسال می‌شود.
  - در غیر این صورت، یک نمونه جمع‌آورنده ایجاد شده و داده‌ها جمع‌آوری می‌شوند. زمان پردازش اندازه‌گیری و نتیجه نهایی به صورت یک DataResponse از طریق Kafka ارسال می‌شود.
  
- **تشخیص خودکار منبع داده:**  
  متد `_detect_data_source(parameters)`، بر اساس پارامترهای دریافتی مانند وجود "title"، "url"، "hashtag" و ...، منبع داده را تشخیص داده و به عنوان DataSource مناسب برمی‌گرداند.
  
- **آماده‌سازی پارامترهای جمع‌آورنده:**  
  متد `_prepare_collector_params(data_source, parameters)`، پارامترهای لازم جهت ایجاد نمونه جمع‌آورنده را آماده می‌کند (مانند تنظیم زبان، آدرس وب، تعداد صفحات و غیره).

- **نمونه Singleton:**  
  - `data_collector_service` به عنوان شیء singleton از کلاس DataCollectorService جهت استفاده در سایر بخش‌های سیستم تعریف شده است.

---

### 3️⃣ سرویس پیام‌رسانی (MessagingService) - فایل `messaging_service.py`
این سرویس وظیفه مدیریت یکپارچه پیام‌رسانی در ماژول Data را بر عهده دارد. از طریق این سرویس، ارتباطات مربوط به جمع‌آوری داده، ارسال نتایج و انتشار متریک‌ها به صورت هماهنگ مدیریت می‌شوند.

#### ویژگی‌ها و عملکردها:
- **راه‌اندازی و آماده‌سازی:**  
  متد `initialize()` اتصال به Kafka را برقرار و از طریق TopicManager اطمینان حاصل می‌کند که موضوعات اصلی (مانند DATA_REQUESTS_TOPIC) وجود دارند. همچنین، سرویس جمع‌آوری داده (data_collector_service) نیز آماده‌سازی می‌شود.
  
- **اجرای سرویس:**  
  متد `run()` سرویس پیام‌رسانی را راه‌اندازی کرده و به صورت همزمان سرویس جمع‌آوری داده را به عنوان یک task اجرا می‌کند. در صورت دریافت سیگنال توقف، task مربوطه لغو و منابع آزاد می‌شود.
  
- **توقف سرویس:**  
  متد `shutdown()` اتصال به Kafka را قطع و سرویس جمع‌آوری داده را متوقف می‌کند.
  
- **ارسال نتیجه به مدل:**  
  متد `send_result_to_model(model_id, data, request_id, response_topic)`:
  - یک DataResponse ایجاد می‌کند و اطلاعات مربوط به نتیجه جمع‌آوری داده (مانند وضعیت موفقیت، داده‌های جمع‌آوری‌شده و مشخصات متادیتا) را تنظیم می‌کند.
  - نتیجه از طریق Kafka به موضوع مربوط به مدل ارسال می‌شود.
  
- **انتشار متریک:**  
  متد `publish_metric(metric_name, metric_data)`، متریک‌های مربوط به عملیات جمع‌آوری داده را به موضوع BALANCE_METRICS_TOPIC ارسال می‌کند.
  
- **پردازش درخواست تست:**  
  متد `process_test_request(query_params)`، امکان تست مستقیم سرویس بدون نیاز به Kafka را فراهم کرده و نتیجه جمع‌آوری داده را بر اساس پارامترهای آزمایشی برمی‌گرداند.
  
- **متد کمکی تولید زمان:**  
  متد `_get_timestamp()`، زمان فعلی را به صورت رشته ISO تولید می‌کند.
  
- **نمونه Singleton:**  
  - `messaging_service` به عنوان یک شیء singleton از کلاس MessagingService تعریف شده است.

---

### 4️⃣ سرویس ارسال نتایج (ResultService) - فایل `result_service.py`
این سرویس مسئول ارسال نتایج جمع‌آوری داده به مدل‌ها می‌باشد. با استفاده از این سرویس، نتایج جمع‌آوری شده پس از پردازش توسط سرویس جمع‌آوری داده به صورت پاسخ (DataResponse) به مدل‌های مربوطه ارسال می‌شوند.

#### ویژگی‌ها و عملکردها:
- **راه‌اندازی اولیه:**  
  متد `initialize()` اتصال به Kafka را برقرار می‌کند تا سرویس آماده ارسال نتایج شود.
  
- **ارسال نتیجه موفق:**  
  متد `send_result(model_id, request_id, data, data_type, data_source, metrics, additional_info)`:
  - یک DataResponse با وضعیت موفقیت ایجاد کرده و اطلاعات جمع‌آوری شده، متادیتا و متریک‌ها را به آن اضافه می‌کند.
  - نتیجه از طریق Kafka به موضوع اختصاصی مدل ارسال شده و در دیکشنری `sent_results` ثبت می‌شود.
  
- **ارسال پیام خطا:**  
  متد `send_error(model_id, request_id, error_message, error_code, data_type, data_source)`:
  - یک پاسخ خطا (DataResponse) ایجاد می‌کند و پیام خطا و کد خطا را تنظیم می‌کند.
  - پیام خطا به موضوع اختصاصی مدل ارسال شده و در سابقه نتایج ثبت می‌شود.
  
- **متد کمکی تولید زمان:**  
  متد `_get_timestamp()`، زمان فعلی را به صورت رشته ISO تولید می‌کند.
  
- **نمونه Singleton:**  
  - `result_service` به عنوان شیء singleton از کلاس ResultService جهت استفاده در سراسر سیستم تعریف شده است.

---

### 5️⃣ صادرسازی سرویس‌ها (__init__.py) - فایل `__init__.py`
این فایل تمامی سرویس‌های مربوط به ماژول Data را برای استفاده در سایر بخش‌های سیستم صادرسازی می‌کند.

#### موارد صادر شده:
- **DataCollectorService** و نمونه آن: `data_collector_service`
- **CollectorManager** و نمونه آن: `collector_manager`
- **MessagingService** و نمونه آن: `messaging_service`
- **ResultService** و نمونه آن: `result_service`

لیست __all__ شامل تمامی این سرویس‌ها جهت استفاده در دیگر ماژول‌ها می‌باشد.

---

## 🔄 تعامل بین اجزا در ماژول Data

1. **ارتباط جمع‌آوری داده و مدیریت جمع‌آورنده‌ها:**
   - **CollectorManager** مسئول ثبت و مدیریت نمونه‌های جمع‌آورنده است؛ به گونه‌ای که در صورت نیاز به استفاده مجدد از یک جمع‌آورنده، از همان نمونه استفاده شود.
   - **DataCollectorService** درخواست‌های جمع‌آوری داده را دریافت کرده و با استفاده از جمع‌آورنده‌های مناسب (ثبت شده در CollectorManager و در خود سرویس) عملیات جمع‌آوری داده را انجام می‌دهد.

2. **ارتباط سرویس‌های پیام‌رسانی و ارسال نتایج:**
   - **MessagingService** به عنوان سرویس مرکزی پیام‌رسانی، ارتباط میان درخواست‌های ورودی و جمع‌آوری داده را مدیریت کرده و در نهایت نتایج جمع‌آوری شده را از طریق **ResultService** به مدل‌ها ارسال می‌کند.
   - **ResultService** وظیفه ارسال نتایج یا پیام‌های خطا را به موضوعات اختصاصی مدل‌ها دارد.

3. **هماهنگی از طریق Kafka و TopicManager:**
   - تمامی سرویس‌ها (DataCollectorService، MessagingService و ResultService) از KafkaService جهت ارسال و دریافت پیام استفاده می‌کنند.
   - TopicManager اطمینان حاصل می‌کند که موضوعات مورد نیاز (برای درخواست، نتایج، متریک‌ها و رویدادها) به درستی ایجاد و مدیریت شوند.

---

## 📊 استراتژی‌های بهینه‌سازی و عملکرد

### 1. بهینه‌سازی جمع‌آوری داده
- **مدیریت نمونه‌های جمع‌آورنده:**  
  استفاده از CollectorManager باعث می‌شود که در صورت وجود نمونه فعال، از آن مجدداً استفاده شود و منابع بهینه مصرف شوند.
  
- **تنظیم پارامترهای اختصاصی:**  
  متدهای `_prepare_collector_params` و `_get_collector_type` با بررسی دقیق پارامترهای درخواست، جمع‌آورنده مناسب را انتخاب و پارامترهای مورد نیاز آن را تنظیم می‌کنند.

### 2. هماهنگی پیام‌رسانی
- **اجرای غیرهمزمان:**  
  استفاده از async/await در تمامی سرویس‌ها باعث می‌شود درخواست‌ها و پاسخ‌ها به صورت غیرهمزمان و همزمان پردازش شوند.
  
- **اشتراک در موضوعات:**  
  سرویس‌های DataCollectorService و MessagingService از طریق KafkaService به موضوعات مرتبط اشتراک می‌گیرند تا داده‌ها به موقع دریافت و پردازش شوند.

### 3. مدیریت خطا و پایداری
- **اعتبارسنجی و ارسال پاسخ خطا:**  
  در صورت بروز خطا در جمع‌آوری داده یا پردازش درخواست، پیام‌های خطا از طریق ResultService به مدل‌ها ارسال شده و در لاگ‌ها ثبت می‌شوند.
  
- **ثبت سابقه نتایج:**  
  ResultService با ثبت نتایج ارسال‌شده (موفق یا خطا) در دیکشنری `sent_results` امکان ردیابی دقیق عملکرد و پاسخگویی به درخواست‌ها را فراهم می‌کند.

---
