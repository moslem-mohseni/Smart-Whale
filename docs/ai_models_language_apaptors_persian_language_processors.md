```
ai/models/language/adaptors/persian/
├── core/                                # پردازش‌های پایه و بهینه‌سازی اولیه فارسی
│   ├── analyzer/                        # تحلیل‌های اولیه متن فارسی
│   │   ├── text_analyzer.py             # تحلیل کلی متن فارسی
│   │   └── structure_analyzer.py        # تحلیل ساختار جملات فارسی
│   ├── processor/                       # پردازشگرهای اصلی فارسی
│   │   ├── text_normalizer.py           # نرمال‌سازی متن فارسی
│   │   ├── quantum_vectorizer.py        # بردارسازی کوانتومی متن فارسی
│   │   └── adaptive_pipeline.py         # پایپلاین پردازش تطبیقی فارسی
│   ├── generator/                       # تولیدکننده پاسخ فارسی
│   │   ├── response_generator.py        # تولید پاسخ اصلی فارسی
│   │   └── quantum_pipeline.py          # پایپلاین کوانتومی تولید پاسخ فارسی
│   └── optimizer/                       # بهینه‌سازهای پردازش فارسی
│       ├── quantum_compressor.py        # فشرده‌سازی داده‌های فارسی
│       └── quantum_allocator.py         # تخصیص منابع پردازشی فارسی
│
├── language_processors/                 # پردازش‌های تخصصی زبان فارسی
│   ├── analyzer/                        # تحلیل‌های زبانی مختلف فارسی
│   ├── contextual/                      # مدیریت و تحلیل زمینه فارسی
│   ├── dialects/                        # تشخیص و پردازش گویش‌های فارسی
│   ├── domain/                          # مدیریت دانش دامنه‌های فارسی
│   ├── grammar/                         # تحلیل گرامری متن فارسی
│   ├── literature/                      # تحلیل ادبیات فارسی
│   ├── proverbs/                        # مدیریت ضرب‌المثل‌های فارسی
│   ├── semantics/
│   └── utils/                       # تحلیل معنایی عمیق متن فارسی
│
├── config/                              # تنظیمات اختصاصی زبان فارسی
│   ├── default_config.py                # تنظیمات پیش‌فرض فارسی
│   └── persian_config.py  
│
├── self_learning/                       # (برای هماهنگی با ماژول مرکزی learning، از این پوشه به صورت ارجاعی استفاده می‌شود)
│
├── smart_model.py                       # مدل دانش‌آموز فارسی (با استفاده از سامانه یادگیری مرکزی)
├── teacher.py                           # مدل معلم فارسی (در آینده حذف خواهد شد)
└── language_processor.py                # رابط اصلی پردازش زبان فارسی
```



---

## ۱. هدف ماژول utils

این پوشه شامل ابزارهای کمکی برای پردازش متون فارسی است. عملکرد اصلی ابزارهای این پوشه عبارتند از:
- **نرمال‌سازی متن**: حذف نویزهای متنی، جایگزینی کاراکترهای نامناسب و اصلاح فاصله‌گذاری به کمک کلاس TextNormalizer.
- **توکنیزیشن**: تفکیک متن به جملات و کلمات با استفاده از کلاس Tokenizer؛ در صورت موجود بودن کتابخانه hazm، از آن استفاده می‌شود و در غیر این صورت نسخه fallback پیشرفته به کار گرفته می‌شود.
- **الگوهای regex و توابع کمکی مرتبط**: شامل توابع استخراج الگوهای مشخص (مانند ایمیل، URL، شماره تلفن و...)، حذف تکرار کاراکترها و پاکسازی متن برای مقایسه.
- **توابع و ابزارهای متفرقه (misc)**: شامل توابع تبدیل تاریخ (مثلاً تبدیل تاریخ شمسی به میلادی و بالعکس)، محاسبه آمار، فرمت‌دهی تاریخ و پارس JSON.

---

## ۲. ساختار داخلی پوشه utils

ساختار پیشنهادی پوشه به صورت زیر است:

```
persian/language_processors/utils/
├── __init__.py               # ادغام و واردات ابزارهای utils به صورت مرکزی
├── text_normalization.py     # کلاس TextNormalizer و توابع مرتبط با نرمال‌سازی متن
├── tokenization.py           # کلاس Tokenizer و توابع مربوط به تفکیک جملات و کلمات
├── regex_utils.py            # توابع استخراج الگوهای regex و الگوهای مشترک متنی
└── misc.py                   # توابع کمکی متفرقه (تبدیل تاریخ، محاسبه آمار، فرمت‌دهی تاریخ، پارس JSON و ...)
```

در این ساختار تمامی فایل‌های مربوط به پردازش اولیه و آماده‌سازی متن در یک نقطه متمرکز شده‌اند تا از تکرار کد و پیچیدگی‌های اضافه جلوگیری شود.

---

## ۳. مستندات فایل‌های داخلی

### 3.1. فایل **text_normalization.py**

#### هدف:
- ارائه یک کلاس **TextNormalizer** برای نرمال‌سازی متن‌های فارسی.
- استفاده از کتابخانه hazm در صورت موجودیت؛ در غیر این صورت، استفاده از پیاده‌سازی داخلی پیشرفته که کیفیت نرمال‌سازی را به حد مطلوب نزدیک می‌کند.

#### ساختار کلاس:
- **متد __init__**:  
  - در این متد تلاش می‌شود کتابخانه hazm بارگذاری شود. اگر hazm موجود باشد، از آن برای نرمال‌سازی استفاده می‌شود؛ در غیر این صورت، از متد _fallback_normalize بهره گرفته می‌شود.
  - الگوهای جایگزینی کاراکترها (مانند تبدیل ی و ک عربی به فارسی و تبدیل اعداد انگلیسی به فارسی) و الگوی حذف فاصله‌های اضافی تنظیم می‌شوند.

- **متد normalize(text: str) -> str**:  
  - ورودی: یک متن فارسی.
  - خروجی: متن نرمال‌شده.
  - منطق: ابتدا در صورت موجود بودن hazm از آن برای نرمال‌سازی استفاده می‌کند؛ در صورت بروز خطا یا عدم وجود hazm، به متد _fallback_normalize مراجعه می‌کند. سپس بهبودهای تکمیلی مانند حذف فاصله‌های اضافی انجام می‌شود.

- **متد _fallback_normalize(text: str) -> str**:  
  - پیاده‌سازی داخلی برای نرمال‌سازی در غیاب hazm.
  - شامل جایگزینی کاراکترهای نادرست، اصلاح فاصله‌گذاری و حذف علائم ناخواسته می‌باشد.

---

### 3.2. فایل **tokenization.py**

#### هدف:
- فراهم کردن کلاس **Tokenizer** جهت تفکیک متن فارسی به جملات و کلمات.
- استفاده از کتابخانه hazm به عنوان گزینه اصلی؛ در صورت عدم دسترسی، از نسخه fallback (توابع ساده داخلی) بهره می‌گیرد.

#### ساختار کلاس:
- **متد __init__**:
  - در صورت موجود بودن hazm، از کلاس‌های HazmSentenceTokenizer و HazmWordTokenizer استفاده می‌شود.
  - در غیر این صورت، متدهای _simple_sentence_tokenizer و _simple_word_tokenizer به عنوان fallback تنظیم می‌شوند.
  
- **متد tokenize_sentences(text: str) -> list**:
  - ورودی: یک متن.
  - خروجی: لیستی از جملات.
  - از تابع tokenize() از شیء sentence_tokenizer استفاده می‌کند.

- **متد tokenize_words(text: str) -> list**:
  - ورودی: یک متن.
  - خروجی: لیستی از کلمات.
  - از تابع tokenize() از شیء word_tokenizer استفاده می‌کند.

- **متدهای استاتیک _simple_sentence_tokenizer و _simple_word_tokenizer**:
  - پیاده‌سازی‌های ساده برای تفکیک جملات (با استفاده از علائم پایان جمله) و کلمات (حذف علائم نگارشی و تقسیم بر اساس فاصله).

---

### 3.3. فایل **regex_utils.py**

#### هدف:
- فراهم آوردن توابع و الگوهای regex مشترک برای استخراج اطلاعات از متن‌های فارسی.
- شامل الگوهایی برای تشخیص ایمیل، URL، شماره تلفن ایرانی، تاریخ شمسی، زمان، مبالغ پولی و درصد است.

#### ساختار فایل:
- **دیکشنری PATTERNS**:
  - شامل کلیدهایی مانند EMAIL، URL، PHONE_IR، DATE_PERSIAN، TIME، MONEY و PERCENT.
  - برای هر کلید، الگوی regex و توضیح مختصری داده شده است.

- **متد extract_pattern(text: str, pattern_key: str) -> List[Dict[str, any]]**:
  - ورودی: متن و کلید الگو.
  - خروجی: لیستی از دیکشنری‌هایی که شامل مقدار پیدا شده، اندیس شروع و پایان و نام الگو هستند.

- **متد extract_all_patterns(text: str, pattern_keys: List[str] = None) -> Dict[str, List[Dict[str, any]]**:
  - ورودی: متن و (اختیاری) لیست کلیدهای الگو.
  - خروجی: دیکشنری که در آن کلیدها نام الگوها و مقادیر لیستی از نتایج استخراج شده هستند.
  - در صورت عدم ارسال لیست، تمام الگوهای موجود بررسی می‌شوند.

- **متد remove_repeated_chars(text: str, threshold: int = 3) -> str**:
  - ورودی: متن و تعداد مجاز تکرار (threshold).
  - خروجی: متنی که کاراکترهای تکراری بیش از threshold کاهش یافته‌اند.

- **متد cleanup_text_for_compare(text: str) -> str**:
  - ورودی: متن.
  - خروجی: متنی ساده‌شده (حذف علائم نگارشی غیرضروری و چند فاصله) برای مقایسه راحت‌تر.

---

### 3.4. فایل **misc.py**

#### هدف:
- ارائه توابع متفرقه و ابزارهای کمکی جهت پردازش‌های عمومی متن.
- شامل توابع تبدیل تاریخ (jalali_to_gregorian و gregorian_to_jalali)، محاسبه آمار (compute_statistics)، فرمت‌دهی تاریخ (format_datetime) و پارس JSON (parse_json) می‌باشد.

#### ساختار فایل:
- **تلاش برای بارگذاری کتابخانه jdatetime**:
  - در صورت موفقیت، از آن استفاده می‌شود؛ در غیر این صورت یک کلاس DummyJDateTime و فضای نامی DummyJdatetimeModule تعریف شده تا عملکرد تقریبی تاریخ شمسی-میلادی فراهم شود.
  
- **متد jalali_to_gregorian(jalali_date: str, fmt: str = "%Y/%m/%d") -> Optional[datetime]**:
  - ورودی: رشته تاریخ شمسی و فرمت آن.
  - خروجی: تاریخ میلادی به‌صورت شیء datetime.
  
- **متد gregorian_to_jalali(greg_date: datetime, fmt: str = "%Y/%m/%d") -> Optional[str]**:
  - ورودی: شیء datetime میلادی و فرمت خروجی.
  - خروجی: تاریخ شمسی به‌صورت رشته.
  - در اینجا خطای مربوط به "Unexpected argument" باید بررسی و اصلاح شود؛ (پارامتر ورودی به متد fromgregorian باید به صورت موقعیتی باشد و نام آن "dt" نباشد یا مستند شود).
  
- **متد compute_statistics(values: List[float]) -> Dict[str, Any]**:
  - ورودی: لیستی از اعداد.
  - خروجی: دیکشنری شامل میانگین، میانه، واریانس و انحراف معیار (با گرد کردن به ۴ رقم اعشار).
  
- **متد format_datetime(dt: datetime, fmt: str = "%Y-%m-%d %H:%M:%S") -> str**:
  - فرمت‌دهی شیء datetime به رشته با فرمت دلخواه.
  
- **متد parse_json(json_string: str) -> Optional[Dict[str, Any]]**:
  - پارس کردن رشته JSON به دیکشنری؛ در صورت بروز خطا، None برگردانده می‌شود.

---

### 3.5. فایل **__init__.py**

#### هدف:
- ادغام و صادرسازی (export) تمامی ابزارهای موجود در پوشه utils به صورت مرکزی.
- فایل __init__.py تمامی کلاس‌ها و توابع اصلی (TextNormalizer، Tokenizer، extract_pattern، extract_all_patterns، remove_repeated_chars، cleanup_text_for_compare، jalali_to_gregorian، gregorian_to_jalali، compute_statistics، format_datetime، parse_json) را در لیست __all__ قرار می‌دهد تا در سایر بخش‌های پروژه به راحتی از آن‌ها استفاده شود.

---

## ۴. خلاصه جریان داده (Data Flow) در ماژول utils

1. **ورود متن**: ابتدا متنی که قرار است پردازش شود (مثلاً برای نرمال‌سازی یا توکنیزیشن) به یکی از کلاس‌های اصلی (TextNormalizer یا Tokenizer) ارسال می‌شود.
2. **پردازش اولیه**: در کلاس TextNormalizer، متن ابتدا با استفاده از hazm (در صورت موجودیت) نرمال‌سازی می‌شود؛ در صورت بروز خطا یا عدم دسترسی، از نسخه fallback استفاده می‌شود.
3. **تفکیک متن**: پس از نرمال‌سازی، متن به کمک کلاس Tokenizer به جملات و کلمات تفکیک می‌شود.
4. **استخراج اطلاعات**: در فایل regex_utils.py، الگوهای تعریف‌شده روی متن اجرا شده و اطلاعاتی مانند ایمیل، URL، شماره تلفن و غیره استخراج می‌شوند.
5. **توابع کمکی**: توابع موجود در misc.py برای تبدیل تاریخ، محاسبه آمار، فرمت‌دهی تاریخ و پارس JSON به کار گرفته می‌شوند.
6. **صادرات مرکزی**: __init__.py تمامی این ابزارها را صادر می‌کند تا سایر بخش‌های پروژه بتوانند به‌راحتی از آن‌ها استفاده کنند.

---



در ادامه یک داکیومنت جامع برای پوشه **contextual** ارائه شده که به جریان ورودی و خروجی، عملکرد و تعامل زیرسیستم‌های موجود در این پوشه اشاره می‌کند. این داکیومنت جهت آشنایی تیم توسعه و استفاده‌کنندگان از این ماژول تهیه شده است.

---

# مستندات پوشه contextual

## مقدمه

پوشه **contextual** زیرسیستمی در چارچوب مدل مرجع زبان فارسی است که مسئول پردازش و مدیریت زمینه (Context) مکالمات فارسی می‌باشد. این زیرسیستم با دریافت ورودی‌هایی مانند شناسه مکالمه، پیام فعلی و تاریخچه پیام‌ها، اطلاعات زمینه‌ای جامعی شامل تحلیل نیت کاربر، استخراج موضوعات، دانش‌های مرتبط، موجودیت‌ها و ارزیابی کیفیت کلی زمینه را تولید می‌کند.

تمامی وظایف مرتبط با انتشار پیام‌ها، ذخیره‌سازی در پایگاه داده و ثبت متریک‌های دقیق در سطح بالاتر (مانند فایل language_processor.py) مدیریت می‌شوند. این ماژول تنها مسئول استخراج، تحلیل و یکپارچه‌سازی اطلاعات زمینه‌ای است.

---

## جریان داده (Data Flow)

### ورودی‌ها:
- **شناسه مکالمه:** یک شناسه یکتا که هر مکالمه را مشخص می‌کند.
- **پیام فعلی:** متن پیام ورودی که نیاز به پردازش و تحلیل دارد.
- **تاریخچه پیام‌ها:** لیستی از پیام‌های قبلی مکالمه به همراه اطلاعاتی مانند شناسه پیام، محتوا، زمان ارسال و (در صورت موجود) شاخص‌هایی مانند اهمیت و شباهت.

### پردازش‌ها:
1. **تنظیمات و ثابت‌ها (config.py):**
   - تعریف ثابت‌هایی مانند Context Types، Knowledge Types، Relation Types، Importance Levels و Confidence Levels.
   - توابع کمکی برای دسترسی به این تنظیمات به‌صورت دیکشنری.

2. **مدیریت مکالمه (conversation.py):**
   - ایجاد مکالمه جدید و افزودن پیام به تاریخچه.
   - ذخیره‌سازی اطلاعات مکالمه در حافظه داخلی (in-memory store).
   - تولید یک خروجی اولیه شامل اطلاعات مکالمه مانند شناسه، عنوان، نوع و تعداد پیام‌ها.

3. **پردازش پیام (message_processor.py):**
   - دریافت ورودی‌های پردازشی شامل شناسه مکالمه، پیام فعلی و تاریخچه پیام‌ها.
   - استفاده از ابزارهای NLP (TextNormalizer و Tokenizer) جهت نرمال‌سازی و تفکیک متن.
   - تحلیل نیت کاربر از پیام فعلی (با کمک الگوریتم‌های مبتنی بر الگوهای متنی و توابع ساده).
   - رتبه‌بندی پیام‌های تاریخچه بر اساس شباهت محتوایی با پیام فعلی.
   - استخراج دانش‌های مرتبط (مانند تاریخ، مبالغ و درصدها) با استفاده از توابع استخراج الگوهای regex.
   - استخراج موجودیت‌های مرتبط (مانند ایمیل‌ها، URLها و شماره تلفن‌ها).
   - ارزیابی کیفیت زمینه (Context) بر اساس معیارهایی مانند تعداد پیام‌های مرتبط، اهمیت و شباهت.

4. **تحلیل نیت (intent_analyzer.py):**
   - تحلیل جامع نیت پیام‌های ورودی شامل تعیین نوع نیت (QUESTION, REQUEST, COMMAND, INFORM, STATEMENT)، استخراج موضوعات کلیدی، فوریت پیام، حالت احساسی و وابستگی به زمینه.
   - استفاده از ابزارهای NLP و الگوریتم‌های تطبیقی مبتنی بر الگوهای متنی.
   - در صورت نیاز، امکان استفاده از مدل‌های هوشمند (smart_model یا teacher) جهت بهبود تحلیل در نظر گرفته شده است.

5. **ارزیابی کیفیت زمینه (evaluator.py):**
   - محاسبه امتیاز کیفیت کلی زمینه (Context Quality) بر اساس ترکیب شاخص‌هایی مانند میانگین اهمیت پیام‌ها، شباهت میان پیام‌ها و تطابق موضوعات.
   - رتبه‌بندی پیام‌های زمینه و تولید گزارش‌های جامع شامل متریک‌ها و پیشنهادات بهبود.

6. **ابزارهای پردازش زبان (nlp_tools.py):**
   - ارائه رابطی جامع (NLPEngine) جهت استفاده از توابع نرمال‌سازی، تفکیک جملات و کلمات.
   - فراهم کردن امکانات پردازش متن برای سایر زیرسیستم‌ها.

7. **توابع کمکی (utils.py):**
   - ارائه توابع و ابزارهای کمکی عمومی مانند توابع regex، حذف تکرار کاراکترها، پاکسازی متن جهت مقایسه و سایر عملیات پردازشی تکمیلی.

### خروجی نهایی:
خروجی ماژول **contextual** یک دیکشنری یکپارچه (Context) است که شامل موارد زیر می‌باشد:
- **conversation_info:** اطلاعات کلی مکالمه (شناسه، عنوان، نوع، زمان شروع و تعداد پیام‌ها).
- **context_messages:** لیستی از پیام‌های مرتبط به عنوان زمینه (انتخاب‌شده از تاریخچه پیام‌ها بر اساس رتبه‌بندی).
- **user_intent:** تحلیل نیت کاربر شامل نوع نیت، موضوعات کلیدی، فوریت و وضعیت احساسی.
- **related_knowledge:** لیستی از دانش‌های استخراج‌شده مرتبط با پیام (مثلاً تاریخ‌ها، مبالغ و درصدها).
- **related_entities:** موجودیت‌های استخراج‌شده (مثلاً ایمیل‌ها، URLها، شماره تلفن‌ها).
- **evaluation:** اطلاعات ارزیابی شامل زمان پردازش، امتیاز کیفیت زمینه و تعداد آیتم‌های استخراج‌شده.
- **success:** وضعیت موفقیت عملیات (True/False).

---

## ساختار پوشه contextual

```
contextual/
├── __init__.py                   # صادرسازی API‌های اصلی زیرسیستم contextual
├── config.py                     # تنظیمات و ثابت‌های مربوط به زمینه (Context)
├── conversation.py               # مدیریت مکالمه (ایجاد، افزودن پیام، تولید Context)
├── evaluator.py                  # ارزیابی کیفیت زمینه و رتبه‌بندی پیام‌های مرتبط
├── intent_analyzer.py            # تحلیل جامع نیت پیام‌ها با استخراج موضوعات، فوریت، حالت احساسی و وابستگی به زمینه
├── message_processor.py          # پردازش پیام‌ها و استخراج اطلاعات زمینه‌ای (Context Extraction)
├── nlp_tools.py                  # ابزارهای پردازش زبان طبیعی (NLPEngine) جهت نرمال‌سازی و توکنیزیشن
└── utils.py                      # توابع و ابزارهای کمکی عمومی (مانند regex، پاکسازی متن و …)
```

---

## توضیحات تکمیلی

- **زیرسیستم Contextual:** این ماژول به عنوان یک زیرسیستم پردازش زمینه تعریف شده و ورودی‌های آن (شناسه مکالمه، پیام فعلی و تاریخچه پیام‌ها) در یک فرآیند یکپارچه تجزیه و تحلیل می‌شوند. خروجی تولید شده سپس در سطح بالاتر (مانند language_processor) برای تولید پاسخ‌های نهایی استفاده می‌شود.

- **مدیریت جریان داده:** 
  - ابتدا ورودی‌ها از طریق ماژول‌های NLP پردازش شده (نرمال‌سازی و توکنیزیشن) و سپس توسط ConversationManager مدیریت می‌شوند.
  - MessageProcessor اطلاعات زمینه‌ای مانند نیت کاربر، دانش و موجودیت‌های مرتبط را استخراج می‌کند.
  - IntentAnalyzer نیز تحلیل جامع نیت پیام را انجام می‌دهد.
  - Evaluator کیفیت زمینه تولید شده را ارزیابی کرده و گزارش‌هایی شامل متریک‌ها و پیشنهادات بهبود فراهم می‌کند.

- **یکپارچه‌سازی خروجی:**  
  تمامی بخش‌ها در نهایت یک خروجی یکپارچه (Context) تولید می‌کنند که شامل اطلاعات مکالمه، تحلیل نیت کاربر، پیام‌های زمینه‌ای انتخاب‌شده، دانش و موجودیت‌های مرتبط به همراه ارزیابی کیفیت است.

- **انعطاف‌پذیری و توسعه‌پذیری:**  
  طراحی این زیرسیستم به‌گونه‌ای است که در صورت نیاز به استفاده از مدل‌های هوشمند (smart_model یا teacher) یا توسعه الگوریتم‌های پیچیده‌تر، تغییرات لازم با حداقل تغییر در ساختار کلی اعمال می‌شود.

---

