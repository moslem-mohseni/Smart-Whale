# **مستندات `models/base_model/`**

## **📌 معرفی (`Introduction`)**
`base_model` شامل ماژول‌های موردنیاز برای فاین‌تیونینگ و پردازش مدل **ParsBERT** برای زبان فارسی است. این ماژول شامل **بارگذاری مدل، آموزش، ارزیابی، تنظیمات، مدیریت داده‌ها، بهینه‌سازی و ذخیره Embeddingها** است.

**ویژگی‌های اصلی:**
- بارگذاری و تنظیم `ParsBERT`
- مدیریت داده‌های آموزشی و پردازش اولیه
- پیاده‌سازی فرآیند آموزش و ارزیابی مدل
- بهینه‌سازی مدل با `AdamW` و `SGD`
- مدیریت `Milvus` برای ذخیره Embeddingهای معنایی

---
## **📂 ساختار پروژه (`Project Structure`)**
```
models/
├── base_model/
├    ├── __init__.py               # مقداردهی اولیه ماژول
├    ├── config.py                 # تنظیمات مدل
├    ├── dataset.py                # مدیریت داده‌های آموزشی
├    ├── model.py                  # بارگذاری و مدیریت ParsBERT
├    ├── trainer.py                # اجرای فرآیند آموزش مدل
├    ├── evaluator.py              # ارزیابی عملکرد مدل
├    ├── optimizer.py              # مدیریت بهینه‌سازها
├    ├── loss.py                   # تعریف توابع هزینه
├    ├── feature_extractor.py      # استخراج ویژگی‌های معنایی
├    ├── embedding_manager.py      # مدیریت Embeddingها در Milvus
├    ├── utils.py                  # ابزارهای کمکی
├
├── advance_model/                 # در حال توسعه
├── other_model/
├── other_model/
├── other_model/
├── other_model/

```

---
## **⚙️ تنظیمات (`Configuration`)**
فایل `config.py` تنظیمات کلی مدل را شامل می‌شود:
- `MODEL_NAME` → نام مدل (`ParsBERT`)
- `BATCH_SIZE` → اندازه `batch`
- `LEARNING_RATE` → نرخ یادگیری مدل
- `EPOCHS` → تعداد `epoch`ها
- `USE_GPU` → فعال‌سازی استفاده از `CUDA`

برای تغییر این تنظیمات، متغیرهای محیطی را مقداردهی کنید یا مستقیماً در `config.py` مقداردهی کنید.

---
## **📂 مدل (`Model`)**
- `model.py` مسئول بارگذاری و مدیریت `ParsBERT` است.
- **کلاس `PersianBERTModel`** شامل توابع زیر است:
  - `tokenize_text()` → تبدیل متن به `token`
  - `predict()` → پیش‌بینی برچسب‌های متن ورودی
  - `save_model()` → ذخیره مدل فاین‌تیون‌شده
  - `load_model()` → بارگذاری مدل ذخیره‌شده

---
## **🛠 آموزش مدل (`Training`)**
- `trainer.py` فرآیند فاین‌تیونینگ `ParsBERT` را مدیریت می‌کند.
- از `optimizer.py` برای تنظیم بهینه‌ساز (`AdamW` یا `SGD`) استفاده می‌شود.
- `loss.py` تابع هزینه (`CrossEntropy`) را مدیریت می‌کند.
- بهترین مدل بر اساس دقت ذخیره می‌شود.

اجرای آموزش:
```bash
python models/base_model/trainer.py
```

---
## **🔍 ارزیابی (`Evaluation`)**
- `evaluator.py` داده‌های تست را بارگذاری کرده و دقت مدل را محاسبه می‌کند.
- معیارهای ارزیابی شامل:
  - `Accuracy`
  - `Precision & Recall`
  - `F1-score`
- برای تست مدل اجرا کنید:
```bash
python models/base_model/evaluator.py
```

---
## **📂 استخراج ویژگی‌ها (`Feature Extraction`)**
- `feature_extractor.py` ویژگی‌های معنایی، دستوری و سبکی را از `knowledge/` دریافت می‌کند.
- از `Milvus` برای جستجوی Embeddingهای معنایی مرتبط استفاده می‌شود.
- تحلیل **گرامری**، **سبکی** و **معنایی** روی متون ورودی انجام می‌شود.

---
## **📂 مدیریت Embeddingها (`Embedding Management`)**
- `embedding_manager.py` داده‌های `Milvus` را مدیریت می‌کند.
- Embeddingهای معنایی ذخیره و در جستجوهای آینده استفاده می‌شوند.
- اجرای ذخیره‌سازی Embedding برای یک متن خاص:
```python
from models.base_model.embedding_manager import EmbeddingManager
manager = EmbeddingManager()
manager.store_embedding("نمونه متن فارسی")
```

---
## **🚀 اجرای نمونه (`Usage Examples`)**
**۱. اجرای آموزش مدل:**
```bash
python models/base_model/trainer.py
```
**۲. اجرای ارزیابی:**
```bash
python models/base_model/evaluator.py
```
**۳. ذخیره و بازیابی Embeddingها:**
```python
from models.base_model.embedding_manager import EmbeddingManager
manager = EmbeddingManager()
print(manager.find_similar_texts("زبان فارسی یک زبان غنی است."))
```

---
## **📌 نتیجه‌گیری و مسیر آینده (`Conclusion & Future Work`)**
✅ `models/base_model/` اکنون یک زیرساخت پایدار برای فاین‌تیونینگ `ParsBERT` است.
🚀 **گام بعدی:** توسعه یک مدل **کاملاً مستقل** از `ParsBERT` با داده‌های بهینه‌شده.
📌 **برای مشارکت در توسعه، مستندات را مطالعه کنید و آزمایش‌های بیشتری انجام دهید!**


## مدل‌های پیشنهادی برای توسعه `models/`

### 2️⃣ `dialogue_models/` - مدل‌های مکالمه و چت‌بات
📌 **هدف:** توسعه مدل‌های پیشرفته برای **چت‌بات‌های هوشمند فارسی، دستیارهای مجازی، و پاسخ‌گویی به سوالات کاربر**.

🔹 **ویژگی‌ها:**
- **`GPT-Farsi`**: توسعه مدل‌های **مکالمه‌ای و دیالوگ‌محور** برای کاربردهای خاص مانند **چت‌بات‌های مالی، خدماتی و آموزشی**.
- **`Retrieval-Augmented Generation (RAG)`**: ترکیب **جستجوی اطلاعات** با تولید پاسخ برای مکالمات دقیق‌تر.
- **`Persona-Based Chatbots`**: مدل‌هایی که **شخصیت و سبک پاسخ‌گویی** را حفظ می‌کنند.

---

### 3️⃣ `speech_to_text/` - تبدیل گفتار به متن
📌 **هدف:** پیاده‌سازی مدل‌های **Speech-to-Text مخصوص فارسی** برای کاربردهایی مانند **دیجیتالی‌سازی محتوا، دستیار صوتی و پردازش صوتی**.

🔹 **ویژگی‌ها:**
- استفاده از مدل‌های **`Wav2Vec2` و `Whisper`** برای **بازشناسی گفتار فارسی**.
- آموزش مدل روی **لهجه‌های مختلف فارسی** مانند **تهرانی، مشهدی، اصفهانی، جنوبی و کردی**.
- **پردازش صوتی متون بلند، جلسات و سخنرانی‌ها**.

---

### 4️⃣ `text_summarization/` - خلاصه‌سازی متن
📌 **هدف:** توسعه مدل‌های **خلاصه‌سازی متن فارسی** برای **تولید چکیده مقالات، اخبار، و خلاصه متون قانونی**.

🔹 **ویژگی‌ها:**
- خلاصه‌سازی **چندسطحی** (کلمات کلیدی، چکیده، خلاصه جمله‌محور).
- خلاصه‌سازی **چندسندی** برای متون طولانی.
- ترکیب خلاصه‌سازی **انتزاعی (`Abstractive`) و استخراجی (`Extractive`)**.

---

### 5️⃣ `sentiment_analysis/` - تحلیل احساسات و نظرات کاربران
📌 **هدف:** توسعه مدل‌های **تحلیل احساسات فارسی** برای **تحلیل نظرات، تشخیص احساس در متون و ردیابی احساسات کاربران**.

🔹 **ویژگی‌ها:**
- تشخیص **احساسات مثبت، منفی و خنثی**.
- **تحلیل پیشرفته‌تر** شامل **خشم، شادی، ترس، تعجب و ناراحتی**.
- کاربرد در **تحلیل نظرات مشتریان، رسانه‌های اجتماعی و نظرات کاربران**.

---

### 6️⃣ `knowledge_graph/` - گراف دانش فارسی
📌 **هدف:** ایجاد **پایگاه داده دانش زبانی فارسی** برای **بهبود پاسخ‌گویی مدل‌ها و افزایش دقت تحلیل معنایی**.

🔹 **ویژگی‌ها:**
- **مدل‌های `Entity Linking` و `Named Entity Recognition (NER)`** برای **استخراج موجودیت‌های فارسی** (نام‌ها، مکان‌ها، رویدادها).
- **ایجاد گراف ارتباطی** بین مفاهیم فارسی.
- **کاربرد در جستجوهای معنایی و پاسخ‌گویی هوشمند**.

---

### 7️⃣ `plagiarism_detection/` - تشخیص سرقت ادبی و پارافریز
📌 **هدف:** توسعه مدل‌هایی که می‌توانند **متون مشابه، بازنویسی‌شده یا سرقت‌شده را تشخیص دهند**.

🔹 **ویژگی‌ها:**
- **تشخیص پارافریز (`Paraphrase Detection`)** در متون فارسی.
- **مقایسه و بررسی مشابهت مقالات، پایان‌نامه‌ها و محتوای آموزشی**.
- **کاربرد در ناشران، دانشگاه‌ها و سامانه‌های بررسی سرقت علمی**.

---

### 8️⃣ `domain_specific_models/` - مدل‌های خاص‌دامنه
📌 **هدف:** توسعه مدل‌های تخصصی برای **حوزه‌های پزشکی، حقوقی، مالی و آموزشی**.

🔹 **حوزه‌ها:**
- **`MedicalBERT`** → پردازش **متون پزشکی فارسی** و استخراج اطلاعات از **مقالات پزشکی**.
- **`LegalBERT`** → پردازش **متون حقوقی فارسی** برای تحلیل **قراردادها، قوانین و مقررات**.
- **`FinancialBERT`** → تحلیل **متون اقتصادی و تجاری** برای **پیش‌بینی روندهای مالی**.

---

### 9️⃣ `low_resource_learning/` - یادگیری با داده کم
📌 **هدف:** توسعه مدل‌هایی که در **شرایط کمبود داده‌های فارسی، دقت بالا داشته باشند**.

🔹 **ویژگی‌ها:**
- **متدهای `Few-Shot` و `Zero-Shot`** برای پردازش زبان فارسی **بدون نیاز به داده زیاد**.
- **انتقال دانش از مدل‌های چندزبانه (`Transfer Learning`)**.
- **کاربرد در پردازش زبان فارسی کم‌منابع و متون با داده‌های محدود**.


## مدل‌های پیشنهادی برای پردازش و تولید اسناد فارسی

### 1️⃣ `document_generation/` - تولید اسناد فارسی
📌 **هدف:** توسعه مدل‌هایی که **به‌صورت خودکار اسناد فارسی را در فرمت‌های مختلف مانند `Word`، `PowerPoint`، `Excel` و `PDF` تولید کنند**.

🔹 **ویژگی‌ها:**
- **تبدیل متن به سند (`Text-to-Document`)**: تولید خودکار اسناد `Word` و `PDF` از روی متن ورودی.
- **قالب‌های هوشمند (`Smart Templates`)**: امکان **تولید اسناد رسمی، گزارش‌ها، و تحقیقات دانشگاهی** با قالب‌های از پیش‌تعریف‌شده.
- **ساختاردهی پویا (`Dynamic Structuring`)**: تشخیص **سربرگ‌ها، فهرست‌ها، جداول، نمودارها و تصاویر** در اسناد.

---

### 2️⃣ `pdf_processing/` - پردازش و ویرایش PDF فارسی
📌 **هدف:** توسعه مدل‌هایی که **متون فارسی را از فایل‌های PDF استخراج، پردازش و ویرایش کنند**.

🔹 **ویژگی‌ها:**
- **`OCR فارسی`**: تشخیص و استخراج متن از **PDF‌های اسکن‌شده**.
- **ویرایش و پردازش (`Text Augmentation`)**: امکان **تصحیح، خلاصه‌سازی و بازنویسی محتوای PDF**.
- **تبدیل فرمت‌ها (`Convert Formats`)**: تبدیل **PDF به Word، Excel و HTML**.

---

### 3️⃣ `powerpoint_ai/` - تولید اسلایدهای فارسی هوشمند
📌 **هدف:** ایجاد **مدلی که بتواند محتوای فارسی را به‌صورت خودکار به `PowerPoint` تبدیل کند**.

🔹 **ویژگی‌ها:**
- **`Text-to-Slide`**: تبدیل متن فارسی به **اسلایدهای استاندارد و حرفه‌ای**.
- **طراحی خودکار (`Auto-Design`)**: انتخاب **فونت، رنگ و قالب‌های مناسب برای پرزنتیشن‌های فارسی**.
- **تولید نمودارها و جداول (`Graph & Table Generator`)**: استخراج داده‌های مهم و تبدیل آن‌ها به **نمودارهای تصویری و جدول‌های سازمان‌یافته**.

---

### 4️⃣ `excel_ai/` - تحلیل و تولید داده‌های اکسل فارسی
📌 **هدف:** توسعه مدلی که **بتواند داده‌های فارسی را در `Excel` مدیریت، پردازش و تحلیل کند**.

🔹 **ویژگی‌ها:**
- **`Data Entry Automation`**: تولید خودکار جداول **بر اساس متن فارسی**.
- **تحلیل داده‌ها (`Data Analysis`)**: پردازش **داده‌های مالی، تجاری و علمی** با هوش مصنوعی.
- **تبدیل متن به اکسل (`Text-to-Excel`)**: تبدیل **گزارش‌های متنی به جداول عددی و گراف‌های آماری**.

---

### 5️⃣ `knowledge_extraction/` - استخراج اطلاعات از اسناد فارسی
📌 **هدف:** توسعه مدلی که **بتواند اطلاعات کلیدی را از اسناد رسمی و پژوهشی فارسی استخراج کند**.

🔹 **ویژگی‌ها:**
- **تشخیص مفاهیم (`Named Entity Recognition - NER`)**: شناسایی **اسامی، تاریخ‌ها، مکان‌ها و سازمان‌ها** در اسناد فارسی.
- **استخراج جداول (`Table Extraction`)**: تبدیل **جداول موجود در PDF و Word به داده‌های قابل پردازش**.
- **تحلیل متن (`Semantic Text Processing`)**: درک **ساختار، روابط و معنای اسناد فارسی**.

---

## **🚀 چرا این مدل‌ها مهم هستند؟**
✅ **اتوماسیون فرآیندهای سازمانی**: بهبود سرعت و دقت در تولید گزارش‌ها، ارائه‌ها و مستندات فارسی.  
✅ **بهینه‌سازی تجربه کاربر**: کاهش زمان تولید محتوا برای کاربران فارسی‌زبان.  
✅ **افزایش دقت پردازش متون فارسی**: استفاده از NLP برای درک و سازمان‌دهی اطلاعات.  
✅ **رقابت با ابزارهای بین‌المللی**: ساخت مدل‌هایی که **جایگزین فارسی‌محور برای ابزارهای Microsoft Office و Google Docs** باشند.  

---

## **🔮 آینده پروژه**
🚀 **گام بعدی:** می‌توان ابتدا **مدل‌های پردازش PDF و Word** را پیاده‌سازی کرد، سپس به سمت **PowerPoint و Excel** گسترش داد.  
📌 **در نهایت، یک سیستم جامع برای پردازش اسناد فارسی** خواهیم داشت که می‌تواند **تمامی نیازهای کاربران و سازمان‌های فارسی‌زبان را پوشش دهد**.


