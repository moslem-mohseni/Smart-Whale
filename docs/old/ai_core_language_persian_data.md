# مستندات ماژول `data`

## 🎯 هدف ماژول
ماژول `data` در پروژه مسئول جمع‌آوری، پردازش و مدیریت داده‌های متنی، صوتی و تصویری برای سیستم پردازش زبان فارسی است. این ماژول با استفاده از یک معماری ماژولار و مقیاس‌پذیر، مدیریت جریان داده از منابع مختلف را امکان‌پذیر می‌سازد. قابلیت‌های اصلی این ماژول شامل:
- جمع‌آوری داده از منابع متنوع (فایل‌ها، وب‌سایت‌ها، شبکه‌های اجتماعی)
- تشخیص و پردازش هوشمند انواع محتوا (متن، تصویر، صوت، ویدیو)
- حفظ روابط بین محتواهای مرتبط
- مدیریت بهینه ذخیره‌سازی و جلوگیری از دوباره‌کاری
- انتشار استاندارد داده‌ها برای پردازش‌های بعدی

📂 **مسیر ماژول:** `ai/data/`

## 📌 ساختار کلی ماژول

```
data/
│── collectors/                        # جمع‌آورنده‌های داده از منابع مختلف
│   ├── base/                         
│   │   ├── collector.py              # کلاس پایه برای تمام collectors
│   │   └── exceptions.py             # استثناهای مخصوص collectors
│   │
│   ├── file/                         # جمع‌آوری از انواع فایل‌ها
│   │   ├── book_collector.py         # پردازش کتاب‌ها (PDF, EPUB, etc)
│   │   └── document_collector.py     # پردازش اسناد (DOCX, TXT, etc)
│   │
│   ├── web/                          # جمع‌آوری از منابع وب
│   │   ├── website_collector.py      # وب‌سایت‌های عمومی
│   │   ├── blog_collector.py         # وبلاگ‌ها
│   │   └── wikipedia_collector.py    # ویکی‌پدیا
│   │
│   └── social/                       # شبکه‌های اجتماعی
│       ├── telegram_collector.py
│       └── twitter_collector.py
│
│── content/                          # مدیریت محتوا و روابط
│   ├── analyzer/                     
│   │   ├── content_analyzer.py       # تشخیص و تجزیه نوع محتوا
│   │   ├── text_analyzer.py          # تحلیل محتوای متنی
│   │   ├── image_analyzer.py         # تحلیل تصاویر
│   │   ├── audio_analyzer.py         # تحلیل صوت
│   │   └── video_analyzer.py         # تحلیل ویدیو
│   │
│   ├── processor/                    
│   │   ├── file_processor.py         # مدیریت فایل‌های چندرسانه‌ای
│   │   ├── hash_manager.py           # مدیریت هش‌ها و شناسه‌های یکتا
│   │   └── batch_processor.py        # پردازش دسته‌ای محتوا
│   │
│   └── relationship/                 
│       ├── content_relationship.py    # مدیریت روابط بین محتواها
│       └── metadata_manager.py        # مدیریت متادیتا
│
│── messaging/                        # مدیریت پیام‌رسانی
│   ├── composer/                     
│   │   ├── message_composer.py       # ساخت پیام‌های استاندارد
│   │   └── topic_manager.py          # مدیریت تاپیک‌های Kafka
│   │
│   ├── publisher/                    
│   │   └── kafka_publisher.py        # انتشار پیام روی Kafka
│   │
│   └── schemas/                      # طرح‌های پیام برای انواع محتوا
│       ├── text_schema.py
│       ├── media_schema.py
│       └── composite_schema.py
│
│── storage/                          # مدیریت ذخیره‌سازی
│   ├── object_storage/               # ذخیره‌سازی فایل‌ها
│   │   └── file_storage.py           # رابط با Object Storage
│   │
│   └── interfaces/                   # رابط‌های ذخیره‌سازی
│       ├── cache_interface.py        # رابط با Redis
│       ├── database_interface.py     # رابط با ClickHouse
│       └── stream_interface.py       # رابط با Kafka
│
└── config/                           # تنظیمات
    ├── collectors_config.py          # تنظیمات collectors
    ├── storage_config.py             # تنظیمات storage
    ├── messaging_config.py           # تنظیمات messaging
    └── content_config.py             # تنظیمات content
```

## 📂 شرح تفصیلی بخش‌ها

### 1️⃣ بخش Collectors
این بخش مسئول جمع‌آوری داده از منابع مختلف است. هر collector از کلاس پایه `BaseCollector` ارث‌بری می‌کند که عملیات مشترک را فراهم می‌کند.

#### 🔹 `base/collector.py`
کلاس پایه برای تمام collectors که شامل:
- متدهای اصلی برای جمع‌آوری داده
- مدیریت خطاها
- اتصال به Content Analyzer
- قابلیت‌های پایه برای پردازش موازی

#### 🔹 `file/book_collector.py`
مسئول پردازش کتاب‌ها با قابلیت‌های:
- پشتیبانی از فرمت‌های مختلف (PDF, EPUB, MOBI)
- استخراج متن، تصاویر و فهرست مطالب
- حفظ ساختار فصل‌بندی
- استخراج متادیتای کتاب

#### 🔹 `web/wikipedia_collector.py`
جمع‌آوری داده از ویکی‌پدیا با قابلیت‌های:
- دریافت مقالات از API ویکی‌پدیا
- پردازش لینک‌های داخلی و خارجی
- استخراج تصاویر مرتبط
- مدیریت redirect‌ها

#### 🔹 `social/telegram_collector.py`
جمع‌آوری داده از تلگرام با قابلیت‌های:
- اتصال به API تلگرام
- پشتیبانی از انواع پیام (متن، تصویر، ویدیو)
- مدیریت گروه‌ها و کانال‌ها
- پردازش پیام‌های forward شده

### 2️⃣ بخش Content
این بخش مسئول تحلیل، پردازش و مدیریت روابط محتواست.

#### 🔹 `analyzer/content_analyzer.py`
تشخیص و تجزیه نوع محتوا با قابلیت‌های:
- تشخیص خودکار نوع محتوا
- تجزیه محتوای ترکیبی به اجزای آن
- استخراج متادیتای پایه
- ارسال به analyzer تخصصی

#### 🔹 `processor/file_processor.py`
مدیریت فایل‌های چندرسانه‌ای با قابلیت‌های:
- محاسبه هش فایل‌ها
- بررسی تکراری بودن
- آپلود به object storage
- مدیریت شناسه‌های یکتا

#### 🔹 `relationship/content_relationship.py`
مدیریت روابط بین محتواها با قابلیت‌های:
- ایجاد گراف ارتباطی بین محتواها
- حفظ اطلاعات زمانی و ترتیبی
- مدیریت محتواهای مرتبط
- پشتیبانی از انواع مختلف رابطه

### 3️⃣ بخش Messaging
این بخش مسئول ساخت و انتشار پیام‌های استاندارد است.

#### 🔹 `composer/message_composer.py`
ساخت پیام‌های استاندارد با قابلیت‌های:
- تبدیل داده به فرمت استاندارد
- اضافه کردن متادیتا
- اعتبارسنجی پیام
- مدیریت محتواهای ترکیبی

#### 🔹 `publisher/kafka_publisher.py`
انتشار پیام روی Kafka با قابلیت‌های:
- انتخاب تاپیک مناسب
- مدیریت partition‌ها
- تضمین تحویل پیام
- پشتیبانی از batch publishing

### 4️⃣ بخش Storage
این بخش مسئول مدیریت ذخیره‌سازی داده‌ها است.

#### 🔹 `object_storage/file_storage.py`
مدیریت ذخیره‌سازی فایل‌ها با قابلیت‌های:
- آپلود و دانلود فایل‌ها
- مدیریت نسخه‌ها
- بهینه‌سازی فضای ذخیره‌سازی
- مدیریت دسترسی‌ها

#### 🔹 `interfaces/cache_interface.py`
رابط با Redis برای کش با قابلیت‌های:
- ذخیره موقت داده‌های پرکاربرد
- مدیریت TTL
- پشتیبانی از الگوهای مختلف کش
- همگام‌سازی کش‌ها

## 🔄 نحوه تعامل بخش‌ها

1. **جریان دریافت داده:**
   - Collector داده را از منبع دریافت می‌کند
   - Content Analyzer نوع محتوا را تشخیص می‌دهد
   - File Processor فایل‌های چندرسانه‌ای را مدیریت می‌کند
   - Relationship Manager روابط را ثبت می‌کند
   - Message Composer پیام استاندارد می‌سازد
   - Kafka Publisher پیام را منتشر می‌کند

2. **مدیریت محتوای ترکیبی:**
   - تشخیص اجزای محتوا
   - پردازش هر جزء با analyzer مربوطه
   - ذخیره فایل‌ها در object storage
   - ایجاد روابط بین اجزا
   - انتشار پیام حاوی تمام اجزا و روابط

## 🛠 قابلیت‌های کلیدی

1. **مقیاس‌پذیری:**
   - امکان اجرای موازی collectors
   - پردازش توزیع‌شده محتوا
   - مدیریت بار روی storage

2. **انعطاف‌پذیری:**
   - افزودن آسان منابع جدید
   - پشتیبانی از انواع جدید محتوا
   - تغییر سیستم‌های ذخیره‌سازی

3. **قابلیت اطمینان:**
   - مدیریت خطاها
   - تضمین تحویل پیام‌ها
   - پشتیبان‌گیری خودکار

4. **بهینه‌سازی:**
   - جلوگیری از ذخیره محتوای تکراری
   - مدیریت هوشمند کش
   - پردازش دسته‌ای برای کارایی بیشتر

## 📝 نکات پیاده‌سازی

1. **مدیریت خطا:**
   ```python
   try:
       content = collector.collect()
       analyzed_content = content_analyzer.analyze(content)
       message = message_composer.compose(analyzed_content)
       publisher.publish(message)
   except CollectorException as e:
       logger.error(f"خطا در جمع‌آوری داده: {e}")
   ```

2. **پردازش موازی:**
   ```python
   async def process_content(content):
       tasks = []
       for item in content.items:
           task = asyncio.create_task(
               content_analyzer.analyze_async(item)
           )
           tasks.append(task)
       return await asyncio.gather(*tasks)
   ```

3. **مدیریت روابط:**
   ```python
   def process_composite_content(content):
       relationship_manager = RelationshipManager()
       
       # پردازش اجزای محتوا
       for component in content.components:
           processed = processor.process(component)
           relationship_manager.add_component(processed)
           
       # ایجاد روابط
       relationship_manager.create_relationships()
   ```

## 🎯 مدیریت حجم داده‌ها

مدیریت حجم داده‌ها یک جنبه مهم از سیستم است. این مدیریت در چند سطح انجام می‌شود:

### سطح Collector
هر collector می‌تواند محدودیت‌های زیر را داشته باشد:
- حداکثر حجم داده در هر درخواست
- حداکثر تعداد درخواست در واحد زمان
- محدودیت حجم کلی در روز

این محدودیت‌ها از طریق فایل تنظیمات `collectors_config.py` قابل تنظیم هستند:

```python
COLLECTOR_LIMITS = {
    "telegram": {
        "max_request_size": "100MB",
        "max_requests_per_minute": 60,
        "daily_volume_limit": "10GB"
    },
    "wikipedia": {
        "max_request_size": "50MB",
        "max_requests_per_minute": 30,
        "daily_volume_limit": "5GB"
    }
}
```

### سطح Storage
سیستم ذخیره‌سازی نیز دارای محدودیت‌های قابل تنظیم است:
- حداکثر حجم هر فایل
- حداکثر حجم کل فایل‌ها
- سیاست‌های نگهداری داده (مدت زمان نگهداری)

## 🔜 نقشه راه توسعه

### فاز اول - زیرساخت پایه
- پیاده‌سازی collectors اصلی (کتاب، ویکی‌پدیا، تلگرام)
- راه‌اندازی سیستم تشخیص محتوا
- پیاده‌سازی مدیریت روابط پایه
- راه‌اندازی انتشار پیام روی Kafka

### فاز دوم - بهبود پردازش
- اضافه کردن پشتیبانی از تصاویر و ویدیو
- بهبود تشخیص روابط محتوا
- پیاده‌سازی پردازش موازی
- بهینه‌سازی عملکرد

### فاز سوم - توسعه منابع
- اضافه کردن منابع جدید (توییتر، وبلاگ‌ها)
- پشتیبانی از فرمت‌های جدید فایل
- بهبود استخراج متادیتا
- پیاده‌سازی فیلترهای پیشرفته

### فاز چهارم - هوشمندسازی
- پیاده‌سازی تشخیص خودکار کیفیت محتوا
- اضافه کردن سیستم طبقه‌بندی خودکار
- بهبود تشخیص محتوای تکراری
- توسعه الگوریتم‌های پیشرفته پردازش

## 💾 سیستم ذخیره‌سازی فایل‌ها

برای مدیریت و ذخیره‌سازی فایل‌ها، پیشنهاد می‌شود از MinIO به عنوان سرویس Object Storage استفاده شود. دلایل این انتخاب:

- سازگاری با API استاندارد S3
- قابلیت اجرا به صورت توزیع‌شده
- پشتیبانی از حجم بالای داده
- امکان مدیریت دسترسی دقیق
- یکپارچگی آسان با سایر سرویس‌ها
- قابلیت‌های مقیاس‌پذیری افقی و عمودی
- پشتیبانی از ویژگی‌های پیشرفته مانند versioning و lifecycle management

این سرویس باید به صورت ماژولار در بخش infrastructure پیاده‌سازی شود تا سایر بخش‌های سیستم بتوانند از طریق یک interface استاندارد با آن تعامل کنند. درست مانند نحوه پیاده‌سازی Kafka و Redis در سیستم.

## 🔄موارد مهم که باید رعایت شوند

اصل تفکیک مسئولیت‌ها (Separation of Concerns) یکی از اصول مهم .در طراحی سیستم‌های نرم‌افزاری است. پس توی طراحی بخش‌های مختلف، باید تفکیک مسئولیت‌ها رو رعایت کنیم. 
استقلال و مدیریت وابستگی و توسعه‌پذیری باید در همه بخش‌ها حفظ بشه!

بیایید وظایف دقیق بخش data را بررسی کنیم:

بخش data فقط باید:
1. داده‌ها را از منابع مختلف دریافت کند
2. داده‌ها را نرمال‌سازی کند (مثل یکسان‌سازی کاراکترها در متن فارسی)
3. داده‌های خام را در فرمت استاندارد تبدیل کند
4. روابط بین داده‌ها را حفظ کند (مثل رابطه بین متن و تصویر در یک پست تلگرام)
5. داده‌ها را در سیستم‌های ذخیره‌سازی مناسب قرار دهد
6. داده‌های آماده شده را روی Kafka منتشر کند تا سایر بخش‌ها بتوانند از آن استفاده کنند

کارهایی که بخش data نباید انجام دهد:
- دسته‌بندی محتوا
- تشخیص کیفیت محتوا
- تشخیص محتوای نامناسب
- پردازش معنایی متن
- تحلیل احساسات
- استخراج موجودیت‌ها
- هر نوع پردازش هوشمند دیگر

این کارها باید توسط ماژول‌های دیگر انجام شود که هر کدام روی تاپیک‌های مربوط به خود در Kafka شنونده هستند و خروجی خود را نیز روی تاپیک‌های دیگری منتشر می‌کنند.
