"""
Ø³Ø±ÙˆÛŒØ³ Ø§ØµÙ„ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø®ØªÙ„Ù Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØªÛŒ
"""
import logging
import asyncio
from typing import Dict, Any, List, Optional, Union, Type

from ai.core.messaging import (
    DataType, DataSource, OperationType, Priority, RequestSource,
    DataRequest, DataResponse, create_data_response,
    kafka_service, TopicManager, DATA_REQUESTS_TOPIC,
    MESSAGE_STATUS_SUCCESS, MESSAGE_STATUS_ERROR
)
from ai.data.collectors.base.collector import BaseCollector
from ai.data.collectors.text.specialized.wiki_collector import WikiCollector
from ai.data.collectors.text.web_collector.general_crawler import GeneralWebCrawler

logger = logging.getLogger(__name__)


class DataCollectorService:
    """
    Ø³Ø±ÙˆÛŒØ³ Ø§ØµÙ„ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ú©Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ø¨Ù‡ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ù‡Ø¯Ø§ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    """

    def __init__(self):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø³Ø±ÙˆÛŒØ³ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡
        """
        self.kafka_service = kafka_service
        self.topic_manager = TopicManager(kafka_service)
        self.request_topic = DATA_REQUESTS_TOPIC
        self._is_initialized = False
        self._shutdown_event = asyncio.Event()

        # Ø«Ø¨Øª Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒâ€ŒØ´Ø¯Ù‡
        self.collectors: Dict[DataSource, Dict[DataType, Type[BaseCollector]]] = {}
        self._register_collectors()

        # Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡â€ŒÙ‡Ø§
        self.active_collectors: Dict[str, BaseCollector] = {}

    def _register_collectors(self):
        """
        Ø«Ø¨Øª Ø§Ù†ÙˆØ§Ø¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒâ€ŒØ´Ø¯Ù‡
        """
        # ØªÙ†Ø¸ÛŒÙ… Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†
        text_collectors = {
            DataSource.WIKI: WikiCollector,
            DataSource.WEB: GeneralWebCrawler,
            # Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø³Ø§ÛŒØ± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
        }

        # Ø«Ø¨Øª Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†
        for source, collector_class in text_collectors.items():
            if source not in self.collectors:
                self.collectors[source] = {}
            self.collectors[source][DataType.TEXT] = collector_class

        # Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø³Ø§ÛŒØ± Ø§Ù†ÙˆØ§Ø¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ù†ÙˆØ§Ø¹ Ø¯Ø§Ø¯Ù‡ Ø¯ÛŒÚ¯Ø± Ø«Ø¨Øª Ú©Ø±Ø¯
        # Ù…Ø§Ù†Ù†Ø¯ Ø¹Ú©Ø³ØŒ ÙˆÛŒØ¯ÛŒÙˆØŒ ØµÙˆØª Ùˆ...

        logger.info(f"âœ… {len(text_collectors)} Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡â€ŒÛŒ Ù…ØªÙ† Ø«Ø¨Øª Ø´Ø¯")

    async def initialize(self):
        """
        Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø³Ø±ÙˆÛŒØ³ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡
        """
        if self._is_initialized:
            return

        # Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ú©Ø§ÙÚ©Ø§
        await self.kafka_service.connect()

        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ù…ÙˆØ¶ÙˆØ¹ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡
        await self.topic_manager.ensure_topic_exists(self.request_topic)

        self._is_initialized = True
        logger.info(f"âœ… Ø³Ø±ÙˆÛŒØ³ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ù‡ Ú©Ø§Ø± Ø§Ø³Øª (Ù…ÙˆØ¶ÙˆØ¹: {self.request_topic})")

    async def run(self):
        """
        Ø§Ø¬Ø±Ø§ÛŒ Ø³Ø±ÙˆÛŒØ³ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø´Ø±ÙˆØ¹ Ú¯ÙˆØ´ Ø¯Ø§Ø¯Ù† Ø¨Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§
        """
        await self.initialize()

        # Ø§Ø´ØªØ±Ø§Ú© Ø¯Ø± Ù…ÙˆØ¶ÙˆØ¹ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡
        group_id = "data-collector-service"
        await self.kafka_service.subscribe(self.request_topic, group_id, self.process_request)

        logger.info(f"ğŸ”” Ø¯Ø± Ø­Ø§Ù„ Ú¯ÙˆØ´ Ø¯Ø§Ø¯Ù† Ø¨Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ø±ÙˆÛŒ Ù…ÙˆØ¶ÙˆØ¹ {self.request_topic}")

        try:
            # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªÙˆÙ‚Ù
            await self._shutdown_event.wait()
        finally:
            # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…Ù†Ø§Ø¨Ø¹
            await self.shutdown()

    async def shutdown(self):
        """
        ØªÙˆÙ‚Ù Ø³Ø±ÙˆÛŒØ³ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¢Ø²Ø§Ø¯Ø³Ø§Ø²ÛŒ Ù…Ù†Ø§Ø¨Ø¹
        """
        if not self._is_initialized:
            return

        logger.info("ğŸ›‘ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ‚Ù Ø³Ø±ÙˆÛŒØ³ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡...")

        # Ø¨Ø³ØªÙ† Ù‡Ù…Ù‡ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„
        for collector_key, collector in list(self.active_collectors.items()):
            await collector.stop_collection()

        # Ù‚Ø·Ø¹ Ø§ØªØµØ§Ù„ Ø§Ø² Ú©Ø§ÙÚ©Ø§
        await self.kafka_service.disconnect()

        self._is_initialized = False
        self._shutdown_event.set()

        logger.info("âœ… Ø³Ø±ÙˆÛŒØ³ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù…ØªÙˆÙ‚Ù Ø´Ø¯")

    async def process_request(self, request_data: Dict[str, Any]):
        """
        Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¯Ø±ÛŒØ§ÙØªÛŒ Ø§Ø² Ù…ÙˆØ¶ÙˆØ¹ Kafka

        :param request_data: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª
        """
        try:
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù†Ù…ÙˆÙ†Ù‡ DataRequest
            request = DataRequest.from_dict(request_data)

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø±Ø®ÙˆØ§Ø³Øª
            operation = request.payload.operation
            model_id = request.payload.model_id
            data_type = request.payload.data_type
            data_source = request.payload.data_source
            parameters = request.payload.parameters
            response_topic = request.payload.response_topic

            # Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ù…Ù„ÛŒØ§Øª
            if isinstance(operation, str):
                try:
                    operation = OperationType(operation)
                except ValueError:
                    operation = OperationType.FETCH_DATA

            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ù†ÙˆØ§Ø¹ Ø¹Ù…Ù„ÛŒØ§Øª
            if operation == OperationType.FETCH_DATA:
                # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡
                await self.collect_data(request, response_topic)
            else:
                # Ø³Ø§ÛŒØ± Ø¹Ù…Ù„ÛŒØ§Øª
                logger.warning(f"âš  Ø¹Ù…Ù„ÛŒØ§Øª '{operation}' Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯")

                # Ø§Ø±Ø³Ø§Ù„ Ù¾Ø§Ø³Ø® Ø®Ø·Ø§
                error_response = create_data_response(
                    request_id=request.metadata.request_id,
                    model_id=model_id,
                    status=MESSAGE_STATUS_ERROR,
                    error_message=f"Ø¹Ù…Ù„ÛŒØ§Øª '{operation}' Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯"
                )

                await self.kafka_service.send_data_response(error_response, response_topic)

        except Exception as e:
            logger.exception(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø±Ø®ÙˆØ§Ø³Øª: {str(e)}")

    async def collect_data(self, request: DataRequest, response_topic: str):
        """
        Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø±Ø®ÙˆØ§Ø³Øª

        :param request: Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¯Ø§Ø¯Ù‡
        :param response_topic: Ù…ÙˆØ¶ÙˆØ¹ Ù¾Ø§Ø³Ø®
        """
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø±Ø®ÙˆØ§Ø³Øª
        model_id = request.payload.model_id
        data_type = request.payload.data_type
        data_source = request.payload.data_source
        parameters = request.payload.parameters

        # ØªØ¨Ø¯ÛŒÙ„ Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ Ø¨Ù‡ Enum
        if isinstance(data_type, str):
            try:
                data_type = DataType(data_type)
            except ValueError:
                data_type = DataType.TEXT

        if isinstance(data_source, str) and data_source:
            try:
                data_source = DataSource(data_source)
            except ValueError:
                data_source = self._detect_data_source(parameters)
        elif not data_source:
            data_source = self._detect_data_source(parameters)

        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡ Ù…Ù†Ø§Ø³Ø¨
        if data_source not in self.collectors or data_type not in self.collectors[data_source]:
            error_message = f"Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ '{data_type}' Ø§Ø² Ù…Ù†Ø¨Ø¹ '{data_source}' ÛŒØ§ÙØª Ù†Ø´Ø¯"
            logger.error(f"âŒ {error_message}")

            # Ø§Ø±Ø³Ø§Ù„ Ù¾Ø§Ø³Ø® Ø®Ø·Ø§
            error_response = create_data_response(
                request_id=request.metadata.request_id,
                model_id=model_id,
                status=MESSAGE_STATUS_ERROR,
                error_message=error_message
            )

            await self.kafka_service.send_data_response(error_response, response_topic)
            return

        try:
            # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡
            collector_class = self.collectors[data_source][data_type]
            collector_key = f"{model_id}:{data_source.value}:{data_type.value}"

            # ØªÙ†Ø¸ÛŒÙ… Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡
            collector_params = self._prepare_collector_params(data_source, parameters)

            # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡
            collector = collector_class(**collector_params)
            self.active_collectors[collector_key] = collector

            # Ø²Ù…Ø§Ù† Ø´Ø±ÙˆØ¹
            import time
            start_time = time.time()

            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡
            data = await collector.collect_data()

            # Ø²Ù…Ø§Ù† Ù¾Ø§ÛŒØ§Ù†
            processing_time = time.time() - start_time

            # Ø­Ø°Ù Ø§Ø² Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„
            if collector_key in self.active_collectors:
                del self.active_collectors[collector_key]

            # Ø¨Ø±Ø±Ø³ÛŒ Ù†ØªÛŒØ¬Ù‡
            if data:
                # Ø³Ø§Ø®Øª Ù¾Ø§Ø³Ø®
                response = create_data_response(
                    request_id=request.metadata.request_id,
                    model_id=model_id,
                    status=MESSAGE_STATUS_SUCCESS,
                    data=data,
                    data_type=data_type,
                    data_source=data_source,
                    metrics={
                        "processing_time_ms": round(processing_time * 1000, 2)
                    }
                )

                # Ø§Ø±Ø³Ø§Ù„ Ù¾Ø§Ø³Ø®
                await self.kafka_service.send_data_response(response, response_topic)
                logger.info(f"âœ… Ø¯Ø§Ø¯Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ '{model_id}' Ø§Ø² Ù…Ù†Ø¨Ø¹ '{data_source}' Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ùˆ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯")
            else:
                # Ø§Ø±Ø³Ø§Ù„ Ù¾Ø§Ø³Ø® Ø®Ø·Ø§
                error_message = f"Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù…Ù†Ø¨Ø¹ '{data_source}' ÛŒØ§ÙØª Ù†Ø´Ø¯"
                error_response = create_data_response(
                    request_id=request.metadata.request_id,
                    model_id=model_id,
                    status=MESSAGE_STATUS_ERROR,
                    error_message=error_message,
                    data_type=data_type,
                    data_source=data_source
                )

                await self.kafka_service.send_data_response(error_response, response_topic)
                logger.warning(f"âš  {error_message}")

        except Exception as e:
            logger.exception(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡: {str(e)}")

            # Ø§Ø±Ø³Ø§Ù„ Ù¾Ø§Ø³Ø® Ø®Ø·Ø§
            error_response = create_data_response(
                request_id=request.metadata.request_id,
                model_id=model_id,
                status=MESSAGE_STATUS_ERROR,
                error_message=f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡: {str(e)}",
                data_type=data_type,
                data_source=data_source
            )

            await self.kafka_service.send_data_response(error_response, response_topic)

    def _detect_data_source(self, parameters: Dict[str, Any]) -> DataSource:
        """
        ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª

        :param parameters: Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª
        :return: Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ø¯Ù‡ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡
        """
        query = parameters.get("query", "")

        # Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ
        if "title" in parameters:
            return DataSource.WIKI
        elif "url" in parameters or query.startswith("http"):
            return DataSource.WEB
        elif "hashtag" in parameters or "username" in parameters:
            return DataSource.TWITTER
        elif "channel" in parameters:
            return DataSource.TELEGRAM
        elif "video_id" in parameters:
            return DataSource.YOUTUBE

        # ØªØ´Ø®ÛŒØµ Ø¨Ø± Ø§Ø³Ø§Ø³ query
        if query.startswith("http"):
            return DataSource.WEB

        # Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        return DataSource.WEB

    def _prepare_collector_params(self, data_source: DataSource, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """
        Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡

        :param data_source: Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ø¯Ù‡
        :param parameters: Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª
        :return: Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±Ù†Ø¯Ù‡
        """
        collector_params = {}

        if data_source == DataSource.WIKI:
            # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ WikiCollector
            collector_params["language"] = parameters.get("language", "fa")
            collector_params["max_length"] = parameters.get("max_length", 5000)
            # ØªÙ†Ø¸ÛŒÙ… Ø¹Ù†ÙˆØ§Ù†
            collector_params["title"] = parameters.get("title", parameters.get("query", ""))

        elif data_source == DataSource.WEB:
            # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ GeneralWebCrawler
            collector_params["source_name"] = "WebCrawler"
            # ØªÙ†Ø¸ÛŒÙ… URL
            start_url = parameters.get("url", parameters.get("query", ""))
            collector_params["start_url"] = start_url
            # ØªÙ†Ø¸ÛŒÙ… ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª
            collector_params["max_pages"] = parameters.get("max_pages", 3)

        # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø³Ø§ÛŒØ± Ù…Ù†Ø§Ø¨Ø¹ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯

        return collector_params


# Ù†Ù…ÙˆÙ†Ù‡ Singleton Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø³Ø±Ø§Ø³Ø± Ø³ÛŒØ³ØªÙ…
data_collector_service = DataCollectorService()
