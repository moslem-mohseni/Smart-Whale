import asyncio
import logging
import aiohttp
from bs4 import BeautifulSoup
from collectors.base.collector import BaseCollector

logging.basicConfig(level=logging.INFO)


class TargetedWebCrawler(BaseCollector):
    """
    Ø®Ø²Ù†Ø¯Ù‡â€ŒÛŒ Ù‡Ø¯ÙÙ…Ù†Ø¯ ÙˆØ¨ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø§Ø² ØµÙØ­Ø§Øª Ù…Ø´Ø®Øµâ€ŒØ´Ø¯Ù‡.
    """

    def __init__(self, source_name: str, target_urls: list):
        super().__init__(source_name)
        self.target_urls = target_urls

    async def collect_data(self):
        """
        Ø§Ø¬Ø±Ø§ÛŒ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø®Ø²Ø´ Ù‡Ø¯ÙÙ…Ù†Ø¯ Ùˆ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡.
        """
        async with aiohttp.ClientSession() as session:
            results = {}
            for url in self.target_urls:
                results[url] = await self._fetch_page(session, url)
            return results

    async def _fetch_page(self, session, url):
        """
        Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØªÙˆØ§ÛŒ ØµÙØ­Ù‡ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡.
        """
        logging.info(f"ğŸ¯ Targeted Crawling: {url}")
        try:
            async with session.get(url) as response:
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                return self._extract_text(soup)
        except Exception as e:
            logging.error(f"âŒ Error fetching {url}: {e}")
            return None

    def _extract_text(self, soup):
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ù…ÙÛŒØ¯ Ø§Ø² ØµÙØ­Ù‡ HTML.
        """
        return '\n'.join([p.get_text() for p in soup.find_all('p')])


if __name__ == "__main__":
    crawler = TargetedWebCrawler("SpecificCrawler", ["https://example.com", "https://example.org"])
    asyncio.run(crawler.start_collection())
