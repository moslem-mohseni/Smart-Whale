import asyncio
import logging
import aiohttp
from bs4 import BeautifulSoup
from collectors.base.collector import BaseCollector

logging.basicConfig(level=logging.INFO)


class GeneralWebCrawler(BaseCollector):
    """
    Ø®Ø²Ù†Ø¯Ù‡â€ŒÛŒ Ø¹Ù…ÙˆÙ…ÛŒ ÙˆØ¨ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø§Ø² ØµÙØ­Ø§Øª HTML.
    """

    def __init__(self, source_name: str, start_url: str, max_pages: int = 5):
        super().__init__(source_name)
        self.start_url = start_url
        self.max_pages = max_pages
        self.visited_urls = set()

    async def collect_data(self):
        """
        Ø§Ø¬Ø±Ø§ÛŒ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø®Ø²Ø´ Ùˆ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡.
        """
        async with aiohttp.ClientSession() as session:
            return await self._crawl(session, self.start_url, 0)

    async def _crawl(self, session, url, depth):
        """
        ØªØ§Ø¨Ø¹ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ø¨Ø±Ø§ÛŒ Ø®Ø²Ø´ Ø¯Ø± ØµÙØ­Ø§Øª ÙˆØ¨.
        """
        if depth >= self.max_pages or url in self.visited_urls:
            return None

        logging.info(f"ğŸŒ Crawling: {url}")
        self.visited_urls.add(url)

        try:
            async with session.get(url) as response:
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                extracted_text = self._extract_text(soup)

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
                links = [a['href'] for a in soup.find_all('a', href=True)]

                # Ø®Ø²Ø´ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ø­Ø¯ÙˆØ¯
                for link in links[:2]:  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ 2 Ù„ÛŒÙ†Ú© Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ø¶Ø§ÙÙ‡â€ŒØ¨Ø§Ø±
                    if link.startswith("http"):
                        await self._crawl(session, link, depth + 1)

                return extracted_text
        except Exception as e:
            logging.error(f"âŒ Error crawling {url}: {e}")
            return None

    def _extract_text(self, soup):
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ù…ÙÛŒØ¯ Ø§Ø² ØµÙØ­Ù‡ HTML.
        """
        return '\n'.join([p.get_text() for p in soup.find_all('p')])


if __name__ == "__main__":
    crawler = GeneralWebCrawler("TestCrawler", "https://example.com", max_pages=3)
    asyncio.run(crawler.start_collection())
