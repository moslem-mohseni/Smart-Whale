# persian/language_processors/contextual/intent_analyzer.py
"""
ูุงฺูู intent_analyzer.py

ุงู ูุงฺูู ุดุงูู ฺฉูุงุณ IntentAnalyzer ุงุณุช ฺฉู ูุธูู ุชุญูู ูุช ูพุงูโูุง ูุฑูุฏ ุจูโุตูุฑุช ุฌุงูุน ู ฺูุฏ ุจุนุฏ ุฑุง ุจุฑ ุนูุฏู ุฏุงุฑุฏ.
ุงู ฺฉูุงุณ ุจุง ุงุณุชูุงุฏู ุงุฒ ุงุจุฒุงุฑูุง ูุฑูุงูโุณุงุฒ (TextNormalizer) ู ุชูฺฉูุฒุดู (Tokenizer) ุงุฒ ูพูุดูโ utilsุ
ุงูฺฏููุง regex ุชุนุฑูโุดุฏู ู ุงูฺฏูุฑุชูโูุง ุชุทุจูุ ูุช ูพุงู (intent type)ุ ููุถูุนุงุช ฺฉูุฏุ ููุฑุชุ ุญุงูุช ุงุญุณุงุณ
ู ูุงุจุณุชฺฏ ุจู ุฒููู (context dependency) ุฑุง ุงุณุชุฎุฑุงุฌ ูโฺฉูุฏ.
"""

import re
import time
from collections import Counter
from typing import List, Dict, Any, Optional

from ..utils.text_normalization import TextNormalizer
from ..utils.tokenization import Tokenizer
from .config import get_confidence_levels

# ุฏุฑ ุงู ูุณุฎู ูโุชูุงู ุฏุฑ ุขูุฏู ูุฏูโูุง ููุดููุฏ (ูุงููุฏ smart_model ุง teacher) ุฑุง ุจูโุนููุงู ูุฑูุฏ ุจู ฺฉูุงุณ ุงุถุงูู ฺฉุฑุฏ
# ุชุง ุชุญูู ุจู ุตูุฑุช ูุจุชู ุจุฑ ุงุฏฺฏุฑ ุงูุฌุงู ุดูุฏ. ุฏุฑ ุญุงู ุญุงุถุฑุ ุงุฒ ุงูฺฏูุฑุชูโูุง ูุจุชู ุจุฑ ุงูฺฏู ุงุณุชูุงุฏู ูโุดูุฏ.


class IntentAnalyzer:
    """
    ฺฉูุงุณ IntentAnalyzer

    ูุธูู ุงู ฺฉูุงุณ ุชุญูู ูุช ูพุงู (intent) ุจู ููุฑุงู ุงุณุชุฎุฑุงุฌ ููุถูุนุงุช ฺฉูุฏุ ููุฑุชุ ุญุงูุช ุงุญุณุงุณ ู ูุงุจุณุชฺฏ ุจู ุฒููู ุฑุง ุฏุงุฑุฏ.
    ุฎุฑูุฌ ููุง ุจู ุตูุฑุช ฺฉ ุฏฺฉุดูุฑ ฺฉูพุงุฑฺู ุงุฑุงุฆู ูโุดูุฏ.
    """
    def __init__(self, model: Optional[Any] = None):
        """
        ููุฏุงุฑุฏู ุงููู ฺฉูุงุณ IntentAnalyzer.

        Args:
            model (Optional[Any]): ุด ูุฏู ููุดููุฏ (ุงุฎุชุงุฑ) ุฌูุช ุจูุจูุฏ ุชุญูู ูุชุ ุฏุฑ ุตูุฑุช ุนุฏู ูุฌูุฏ ุงุฒ ุชุญูู ูุจุชู ุจุฑ ุงูฺฏู ุงุณุชูุงุฏู ูโุดูุฏ.
        """
        self.normalizer = TextNormalizer()
        self.tokenizer = Tokenizer()
        self.model = model  # ุงูฺฉุงู ฺฏุณุชุฑุด ุฏุฑ ุขูุฏู ุจุฑุง ุงุณุชูุงุฏู ุงุฒ ูุฏูโูุง ุงุฏฺฏุฑ

        # ุชุนุฑู ุงูฺฏููุง ุงุตู ุฌูุช ุชุนู ููุน ูุช
        self.question_patterns = [
            r'\?', r'ุ',
            r'\b(ฺู|ฺุฑุง|ฺฉุฌุง|ฺฺฏููู|ฺุทูุฑ|ฺฉ|ฺฉุฏุงู)\b',
            r'\b(ุขุง|ูฺฏุฑ)\b.*\?'
        ]
        self.request_patterns = [
            r'\b(ูุทูุงู?|ุฎูุงูุด|ุชููุง)\b',
            r'\b(ูโุชูุงู|ูโุดูุฏ|ูุดู|ููฺฉูู)\b',
            r'\b(ุจุงุฏ|ูุงุฒ ุฏุงุฑู|ูุฎูุงูู|ูโุฎูุงูู)\b'
        ]
        self.command_patterns = [
            r'^[^ุุ.!?ุ]+[!.]',
            r'\b(ุงูุฌุงู ุจุฏู|ุงุฌุฑุง ฺฉู|ุจุณุงุฒ|ูพุฏุง ฺฉู|ูุญุงุณุจู ฺฉู)\b'
        ]
        self.inform_patterns = [
            r'\b(ูโุฎูุงุณุชู ุจฺฏู|ุงุทูุงุน ุจุฏู|ุจุฏุงูุฏ ฺฉู|ฺฏุฒุงุฑุด ูโุฏูู)\b',
            r'\b(ูฺฉุฑ ูโฺฉูู|ุจู ูุธุฑู|ูุนุชูุฏู|ุญุฏุณ ูโุฒูู)\b'
        ]
        self.urgent_keywords = ["ููุฑ", "ุงุถุทุฑุงุฑ", "ุณุฑุน", "ุนุฌูู", "ุจูุงูุงุตูู", "ุขู", "ูุฑ ฺู ุฒูุฏุชุฑ", "ููุฑุงู", "ููู ุงูุงู", "ุจุฏูู ุชุฃุฎุฑ", "ููุช ฺฉู", "ุถุฑุจโุงูุงุฌู", "ูููุช"]

        # ฺฉููุงุช ุงุญุณุงุณ ูุซุจุช ู ููู
        self.positive_keywords = {"ุฎูุจ", "ุนุงู", "ูููโุงูุนุงุฏู", "ุฎูุดุญุงู", "ุฑุงุถ", "ุฎุฑุณูุฏ", "ุณูพุงุณฺฏุฒุงุฑ", "ููููู", "ูุชุดฺฉุฑ", "ูุฐุช", "ุนุดู", "ุฏูุณุช ุฏุงุดุชู", "ุงูุฏูุงุฑ", "ูููู", "ูพุฑูุฒ", "ูุซุจุช"}
        self.negative_keywords = {"ุจุฏ", "ุงูุชุถุงุญ", "ูุงุฑุงุญุช", "ุนุตุจุงู", "ุฎุดูฺฏู", "ูุชุฃุณู", "ูุงุฑุงุถ", "ุบูฺฏู", "ูุงุงูุฏ", "ุดฺฉุณุช", "ูุงฺฉุงู", "ููุฑุช", "ูุชููุฑ", "ุฎุณุชู", "ูุงูููู", "ููู"}

        # ูุดุงููโูุง ุงุญุณุงุณ (ุจุง ุงูุชุงุฒุฏู)
        self.emotional_markers = {
            "!": 0.2, "!!": 0.4, "!!!": 0.6,
            "ุุ": 0.3,
            "๐": 0.5, "๐ข": 0.5, "๐ก": 0.6
        }

    def analyze(self, message: str, context: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        ุชุญูู ูุช ูพุงู ูุฑูุฏ ุจู ููุฑุงู ุงุณุชุฎุฑุงุฌ ููุถูุนุงุชุ ููุฑุชุ ุญุงูุช ุงุญุณุงุณ ู ูุงุจุณุชฺฏ ุจู ุฒููู.
        ุฏุฑ ุตูุฑุช ุงุฑุงุฆูโ ูพุงูโูุง ุฒูููโุง (context)ุ ูโุชูุงู ุจู ุจุฑุฑุณ ูุงุจุณุชฺฏ ูุฒ ูพุฑุฏุงุฎุช.

        Args:
            message (str): ูุชู ูพุงู ูุฑูุฏ
            context (Optional[List[str]]): ูุณุช ุงุฒ ูพุงูโูุง ูุจู ุง ุฒููู (ุงุฎุชุงุฑ)

        Returns:
            Dict[str, Any]: ุฏฺฉุดูุฑ ุดุงูู:
                - intent_type: ููุน ูุช (QUESTION, REQUEST, COMMAND, INFORM, STATEMENT)
                - topics: ูุณุช ููุถูุนุงุช ฺฉูุฏ ุงุณุชุฎุฑุงุฌโุดุฏู
                - urgency: ุงูุชุงุฒ ููุฑุช (0 ุชุง 1)
                - emotional_state: ุฏฺฉุดูุฑ ุดุงูู ุงูุชุงุฒูุง positiveุ negative ู neutral
                - context_dependency: ุงุทูุงุนุงุช ูุงุจุณุชฺฏ ุจู ุฒููู (is_context_dependent ู dependency_level)
        """
        normalized_message = self.normalizer.normalize(message)
        tokens = self.tokenizer.tokenize_words(normalized_message)

        # ุฏุฑ ุตูุฑุช ูุฌูุฏ ูุฏู ููุดููุฏุ ูโุชูุงู ุงุฒ ุขู ุงุณุชูุงุฏู ฺฉุฑุฏ (ุฏุฑ ุงูุฌุง ุจู ุตูุฑุช ุงุฎุชุงุฑ)
        if self.model:
            # ูุซุงู: ุฎุฑูุฌ ูุฏู ูโุชูุงูุฏ ุดุงูู intentุ topics ู ... ุจุงุดุฏ.
            smart_output = self.model.analyze_intent(normalized_message, context)
            if smart_output:
                return smart_output

        intent_type = self._determine_intent_type(normalized_message)
        topics = self._extract_topics(tokens)
        urgency = self._determine_urgency(normalized_message)
        emotional_state = self._determine_emotional_state(normalized_message)
        context_dependency = self._determine_context_dependency(normalized_message, context)

        return {
            "intent_type": intent_type,
            "topics": topics,
            "urgency": urgency,
            "emotional_state": emotional_state,
            "context_dependency": context_dependency
        }

    def _determine_intent_type(self, message: str) -> str:
        """
        ุชุนู ููุน ูุช ูพุงู ุจุฑ ุงุณุงุณ ุงูฺฏููุง ุงุฒ ูพุด ุชุนุฑูโุดุฏู.

        Args:
            message (str): ูพุงู ูุฑูุงูโุดุฏู

        Returns:
            str: ฺฉ ุงุฒ ุงููุงุน "QUESTION"ุ "REQUEST"ุ "COMMAND"ุ "INFORM" ุง "STATEMENT"
        """
        for pattern in self.question_patterns:
            if re.search(pattern, message, re.IGNORECASE):
                return "QUESTION"
        for pattern in self.request_patterns:
            if re.search(pattern, message, re.IGNORECASE):
                return "REQUEST"
        for pattern in self.command_patterns:
            if re.search(pattern, message, re.IGNORECASE):
                return "COMMAND"
        for pattern in self.inform_patterns:
            if re.search(pattern, message, re.IGNORECASE):
                return "INFORM"
        # ุงฺฏุฑ ูฺ ฺฉุฏุงู ุงุฒ ุงูฺฏููุง ุชุทุจู ูุฏุงุฏูุฏุ ุจุฑุฑุณ ุงุญุณุงุณ ูุฒ ุงูุฌุงู ูโุดูุฏ
        if self._has_emotion(message):
            return "EMOTIONAL"
        return "STATEMENT"

    def _extract_topics(self, tokens: List[str]) -> List[str]:
        """
        ุงุณุชุฎุฑุงุฌ ููุถูุนุงุช ฺฉูุฏ ุงุฒ ูพุงู ุจุฑ ุงุณุงุณ ูุฑุงูุงู ฺฉููุงุช.

        Args:
            tokens (List[str]): ูุณุช ฺฉููุงุช ุงุณุชุฎุฑุงุฌโุดุฏู ุงุฒ ูพุงู

        Returns:
            List[str]: ูุณุช ุงุฒ ฺฉููุงุช ูพุฑุชฺฉุฑุงุฑ ุจูโุนููุงู ููุถูุนุงุช
        """
        # ุญุฐู ฺฉููุงุช ุงุณุช ุณุงุฏู ูุงุฑุณ
        stop_words = {"ู", "ุฏุฑ", "ุจู", "ุงุฒ", "ฺฉู", "ุงู", "ุงุณุช", "ุฑุง", "ุจุง", "ูุง", "ุจุฑุง", "ุขู"}
        filtered_tokens = [token for token in tokens if token.lower() not in stop_words and len(token) > 2]
        if not filtered_tokens:
            return []
        frequency = Counter(filtered_tokens)
        common = frequency.most_common(3)
        return [word for word, count in common]

    def _determine_urgency(self, message: str) -> float:
        """
        ุชุนู ุงูุชุงุฒ ููุฑุช ูพุงู ุจุฑ ุงุณุงุณ ุญุถูุฑ ฺฉููุงุช ู ุนุจุงุฑุงุช ููุฑุช.

        Args:
            message (str): ูพุงู ูุฑูุงูโุดุฏู

        Returns:
            float: ุงูุชุงุฒ ููุฑุช (ุจู 0 ุชุง 1)
        """
        score = 0.0
        lower_message = message.lower()
        for keyword in self.urgent_keywords:
            if keyword.lower() in lower_message:
                score += 0.2
        # ูุญุฏูุฏ ฺฉุฑุฏู ุจู ุจุดุชุฑู ููุฏุงุฑ 1.0
        return min(round(score, 2), 1.0)

    def _determine_emotional_state(self, message: str) -> Dict[str, float]:
        """
        ุชุญูู ุญุงูุช ุงุญุณุงุณ ูพุงู ุจุฑ ุงุณุงุณ ฺฉููุงุช ู ูุดุงููโูุง ุงุญุณุงุณ.

        Args:
            message (str): ูพุงู ูุฑูุงูโุดุฏู

        Returns:
            Dict[str, float]: ุดุงูู ุงูุชุงุฒูุง positiveุ negative ู neutral
        """
        pos_score = 0.0
        neg_score = 0.0
        tokens = self.tokenizer.tokenize_words(message.lower())
        # ุงูุชุงุฒุฏู ุจุฑุงุณุงุณ ฺฉููุงุช ูุซุจุช ู ููู
        for token in tokens:
            if token in self.positive_keywords:
                pos_score += 1.0
            if token in self.negative_keywords:
                neg_score += 1.0
        total = pos_score + neg_score
        if total > 0:
            pos_ratio = pos_score / total
            neg_ratio = neg_score / total
        else:
            pos_ratio = neg_ratio = 0.0
        neutral_ratio = 1.0 - (pos_ratio + neg_ratio)
        return {
            "positive": round(pos_ratio, 2),
            "negative": round(neg_ratio, 2),
            "neutral": round(neutral_ratio, 2)
        }

    def _determine_context_dependency(self, message: str, context: Optional[List[str]]) -> Dict[str, Any]:
        """
        ุงุฑุฒุงุจ ูุงุจุณุชฺฏ ูพุงู ุจู ุฒููู (context)ุ ุงฺฏุฑ ูพุงู ฺฉูุชุงู ุง ุฏุงุฑุง ุถูุงุฑ ุจุฏูู ูุฑุฌุน ุงุณุชุ
        ุงุญุชูุงู ูุงุจุณุชฺฏ ุจู ูฺฉุงููู ูุจู ุงูุฒุงุด ูโุงุจุฏ.

        Args:
            message (str): ูพุงู ูุฑูุงูโุดุฏู
            context (Optional[List[str]]): ูุณุช ุงุฒ ูพุงูโูุง ูุจู ุง ุฒููู

        Returns:
            Dict[str, Any]: ุดุงูู:
                - is_context_dependent (bool)
                - dependency_level (float): ููุฏุงุฑ ุจู 0 ุชุง 1
                - referenced_context (List[str]): ูุณุช ุงุฒ ูพุงูโูุง ุฒูููโุง ูุฑุชุจุท (ุฏุฑ ุตูุฑุช ูุฌูุฏ)
        """
        result = {
            "is_context_dependent": False,
            "dependency_level": 0.0,
            "referenced_context": []
        }
        if not context or len(context) == 0:
            return result

        # ุงฺฏุฑ ูพุงู ุจุณุงุฑ ฺฉูุชุงู ุงุณุชุ ุงุญุชูุงู ูุงุจุณุชฺฏ ุจุงูุง ุงุณุช
        if len(message.split()) < 5:
            result["is_context_dependent"] = True
            result["dependency_level"] = 0.6

        # ุจุฑุฑุณ ุดุจุงูุช ุจุง ูพุงูโูุง ุฒููู
        similarities = []
        for ctx_msg in context:
            sim = self._calculate_similarity(message, ctx_msg)
            similarities.append(sim)
        avg_similarity = sum(similarities) / len(similarities) if similarities else 0.0

        # ุงฺฏุฑ ุดุจุงูุช ูุงูฺฏู ุจุงูุง ุจูุฏุ ูุงุจุณุชฺฏ ุจุดุชุฑ ุงุณุช
        if avg_similarity > 0.3:
            result["is_context_dependent"] = True
            result["dependency_level"] = round(min(avg_similarity + 0.3, 1.0), 2)
            # ุงูุชุฎุงุจ ูพุงูโูุง ุฒูููโุง ุจุง ุดุจุงูุช ุจุงูุง
            referenced = [ctx_msg for ctx_msg, sim in zip(context, similarities) if sim > 0.3]
            result["referenced_context"] = referenced

        return result

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """
        ูุญุงุณุจู ุดุจุงูุช ุฏู ูุชู ุจุง ุงุณุชูุงุฏู ุงุฒ ุงูฺฏูุฑุชู SequenceMatcher.

        Args:
            text1 (str): ูุชู ุงูู
            text2 (str): ูุชู ุฏูู

        Returns:
            float: ููุฏุงุฑ ุดุจุงูุช (0 ุชุง 1)
        """
        from difflib import SequenceMatcher
        return SequenceMatcher(None, text1, text2).ratio()

    def _has_emotion(self, message: str) -> bool:
        """
        ุจุฑุฑุณ ูุฌูุฏ ูุดุงููโูุง ุงุญุณุงุณ ุฏุฑ ูพุงู.

        Args:
            message (str): ูพุงู ูุฑูุงูโุดุฏู

        Returns:
            bool: True ุฏุฑ ุตูุฑุช ูุฌูุฏ ุงุญุณุงุณุ ุฏุฑ ุบุฑ ุงู ุตูุฑุช False
        """
        # ุจุฑุฑุณ ูุฌูุฏ ุงููุฌ
        emoji_pattern = re.compile(
            "[" 
            "\U0001F600-\U0001F64F"  # ุตูุฑุชฺฉโูุง
            "\U0001F300-\U0001F5FF"  # ููุงุฏูุง
            "\U0001F680-\U0001F6FF"  # ุญูู ู ููู
            "\U0001F700-\U0001F77F"  # ุนูุงุฆู
            "\U0001F780-\U0001F7FF"  
            "\U0001F800-\U0001F8FF"  
            "\U0001F900-\U0001F9FF"  
            "\U0001FA00-\U0001FA6F"  
            "\U0001FA70-\U0001FAFF"  
            "\U00002702-\U000027B0"  
            "\U000024C2-\U0001F251"
            "]+"
        )
        if emoji_pattern.search(message):
            return True
        # ุจุฑุฑุณ ุชฺฉุฑุงุฑ ฺฉุงุฑุงฺฉุชุฑ
        if re.search(r'(.)\1{2,}', message):
            return True
        return False

# ููููู ุชุณุช ูุณุชูู
if __name__ == "__main__":
    sample_text = "ุณูุงู! ูุทูุงู ุฑุงูููุง ฺฉู ฺุทูุฑ ูโุชูุงูู ุงู ูุดฺฉู ุฑู ุณุฑุน ุญู ฺฉููุ"
    analyzer = IntentAnalyzer()
    result = analyzer.analyze(sample_text, context=["ูู ุงูุฑูุฒ ุฎู ุฎุณุชู ูุณุชู", "ูุดฺฉู ุงุฒ ูุณูุช ุดุจฺฉู ุงุณุช"])
    print("ูุชุฌู ุชุญูู ูุช:", result)
