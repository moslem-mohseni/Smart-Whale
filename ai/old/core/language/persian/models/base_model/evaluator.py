import torch
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from transformers import AutoModelForSequenceClassification
from models.base_model.config import BaseModelConfig
from models.base_model.dataset import get_dataloader
from models.base_model.model import PersianBERTModel


class Evaluator:
    """
    Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ ParsBERT.
    """

    def __init__(self, num_labels=2):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ù„Ø§Ø³ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ.

        :param num_labels: ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
        """
        self.device = torch.device(BaseModelConfig.DEVICE)

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡
        self.model = PersianBERTModel(num_labels=num_labels).model
        self.model.load_state_dict(torch.load(BaseModelConfig.BEST_MODEL_PATH))
        self.model.to(self.device)
        self.model.eval()

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª
        self.test_dataloader = get_dataloader(BaseModelConfig.TEST_DATA_PATH, BaseModelConfig.BATCH_SIZE, shuffle=False)

    def evaluate(self):
        """
        Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª.
        """
        y_true = []
        y_pred = []

        with torch.no_grad():
            for batch in self.test_dataloader:
                input_ids, labels = batch["input_ids"].to(self.device), batch["label"].to(self.device)

                # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„
                outputs = self.model(input_ids)
                predictions = torch.argmax(outputs.logits, dim=1)

                y_true.extend(labels.cpu().numpy())
                y_pred.extend(predictions.cpu().numpy())

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        accuracy = accuracy_score(y_true, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="weighted")

        print("\nðŸ“Œ **Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„:**")
        print(f"ðŸ”¹ `Accuracy`: {accuracy:.4f}")
        print(f"ðŸ”¹ `Precision`: {precision:.4f}")
        print(f"ðŸ”¹ `Recall`: {recall:.4f}")
        print(f"ðŸ”¹ `F1-score`: {f1:.4f}")

        return {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1}


# ==================== ØªØ³Øª ====================
if __name__ == "__main__":
    evaluator = Evaluator(num_labels=3)
    results = evaluator.evaluate()
