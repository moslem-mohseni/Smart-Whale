import torch
import torch.nn as nn
import torch.optim as optim
from transformers import get_scheduler
from models.base_model.config import BaseModelConfig
from models.base_model.dataset import get_dataloader
from models.base_model.model import PersianBERTModel
from tqdm import tqdm


class Trainer:
    """
    Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ParsBERT.
    """

    def __init__(self, num_labels=2):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ù„Ø§Ø³ Ø¢Ù…ÙˆØ²Ø´.

        :param num_labels: ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
        """
        self.device = torch.device(BaseModelConfig.DEVICE)
        self.model = PersianBERTModel(num_labels=num_labels).model.to(self.device)

        # Ù„ÙˆØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        self.train_dataloader = get_dataloader(BaseModelConfig.TRAIN_DATA_PATH, BaseModelConfig.BATCH_SIZE)
        self.valid_dataloader = get_dataloader(BaseModelConfig.VALIDATION_DATA_PATH, BaseModelConfig.BATCH_SIZE)

        # ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ Ù‡Ø²ÛŒÙ†Ù‡ (Loss Function)
        self.criterion = nn.CrossEntropyLoss()

        # ØªØ¹Ø±ÛŒÙ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² (Optimizer)
        self.optimizer = optim.AdamW(self.model.parameters(), lr=BaseModelConfig.LEARNING_RATE, weight_decay=BaseModelConfig.WEIGHT_DECAY)

        # ØªÙ†Ø¸ÛŒÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªØ¯Ø±ÛŒØ¬ÛŒ (Scheduler)
        self.lr_scheduler = get_scheduler(
            name="linear",
            optimizer=self.optimizer,
            num_warmup_steps=BaseModelConfig.WARMUP_STEPS,
            num_training_steps=len(self.train_dataloader) * BaseModelConfig.EPOCHS,
        )

    def train_epoch(self):
        """
        Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ© `epoch` Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„.
        """
        self.model.train()
        total_loss = 0

        loop = tqdm(self.train_dataloader, desc="ðŸš€ Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…ÙˆØ²Ø´", leave=True)
        for batch in loop:
            input_ids, labels = batch["input_ids"].to(self.device), batch["label"].to(self.device)

            # Ø­Ø°Ù Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ
            self.optimizer.zero_grad()

            # Ø§Ø¬Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
            outputs = self.model(input_ids)
            loss = self.criterion(outputs.logits, labels)

            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ùˆ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§
            loss.backward()
            self.optimizer.step()
            self.lr_scheduler.step()

            # Ø°Ø®ÛŒØ±Ù‡ Ù…Ù‚Ø¯Ø§Ø± loss
            total_loss += loss.item()

            # Ù†Ù…Ø§ÛŒØ´ Ù…Ù‚Ø¯Ø§Ø± Loss Ø¯Ø± tqdm
            loop.set_postfix(loss=loss.item())

        return total_loss / len(self.train_dataloader)

    def evaluate(self):
        """
        Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ.
        """
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for batch in self.valid_dataloader:
                input_ids, labels = batch["input_ids"].to(self.device), batch["label"].to(self.device)
                outputs = self.model(input_ids)
                loss = self.criterion(outputs.logits, labels)

                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚Øª Ù…Ø¯Ù„
                predictions = torch.argmax(outputs.logits, dim=1)
                correct += (predictions == labels).sum().item()
                total += labels.size(0)

                total_loss += loss.item()

        accuracy = correct / total
        return total_loss / len(self.valid_dataloader), accuracy

    def train(self):
        """
        Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„.
        """
        best_accuracy = 0

        for epoch in range(BaseModelConfig.EPOCHS):
            print(f"\nðŸ”¹ **Epoch {epoch + 1}/{BaseModelConfig.EPOCHS}**")

            train_loss = self.train_epoch()
            valid_loss, valid_accuracy = self.evaluate()

            print(f"ðŸ“Œ `Train Loss`: {train_loss:.4f} | `Valid Loss`: {valid_loss:.4f} | `Accuracy`: {valid_accuracy:.4f}")

            # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ù‚Øª
            if valid_accuracy > best_accuracy:
                best_accuracy = valid_accuracy
                self.save_model()

        print("âœ… Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!")

    def save_model(self):
        """
        Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡.
        """
        torch.save(self.model.state_dict(), BaseModelConfig.BEST_MODEL_PATH)
        print(f"âœ… Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø± `{BaseModelConfig.BEST_MODEL_PATH}`")


# ==================== ØªØ³Øª ====================
if __name__ == "__main__":
    trainer = Trainer(num_labels=3)
    trainer.train()
