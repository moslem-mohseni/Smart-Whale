import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from models.base_model.config import BaseModelConfig


class PersianBERTModel:
    """
    Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„ ParsBERT Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ†â€ŒØªÛŒÙˆÙ†ÛŒÙ†Ú¯ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ.
    """

    def __init__(self, num_labels=2):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ø¯Ù„.

        :param num_labels: ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ (Ù…Ø«Ù„Ø§Ù‹ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯ÙˆØ¯ÙˆÛŒÛŒ `2`)
        """
        self.device = torch.device(BaseModelConfig.DEVICE)

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ ParsBERT
        self.model = AutoModelForSequenceClassification.from_pretrained(
            BaseModelConfig.MODEL_NAME, num_labels=num_labels
        ).to(self.device)

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(BaseModelConfig.MODEL_NAME)

    def tokenize_text(self, text, max_length=512):
        """
        ØªÙˆÚ©Ù†ÛŒØ²Ù‡ Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² tokenizer Ù…Ø¯Ù„ ParsBERT.

        :param text: Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ
        :param max_length: Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§
        :return: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø´Ø§Ù…Ù„ `input_ids`, `attention_mask`
        """
        return self.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=max_length,
            return_tensors="pt"
        )

    def predict(self, text):
        """
        Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ú†Ø³Ø¨ ÛŒÚ© Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ.

        :param text: Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ
        :return: Ù„ÛŒØ³Øª Ù†Ù…Ø±Ø§Øª Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
        """
        self.model.eval()
        inputs = self.tokenize_text(text)
        inputs = {key: value.to(self.device) for key, value in inputs.items()}

        with torch.no_grad():
            outputs = self.model(**inputs)

        return torch.softmax(outputs.logits, dim=-1).cpu().numpy()

    def save_model(self, save_path=None):
        """
        Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ ÙØ§ÛŒÙ†â€ŒØªÛŒÙˆÙ†â€ŒØ´Ø¯Ù‡.

        :param save_path: Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ (Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ØªØ¹ÛŒÛŒÙ†ØŒ Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ `BaseModelConfig` Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
        """
        save_path = save_path or BaseModelConfig.MODEL_SAVE_PATH
        self.model.save_pretrained(save_path)
        self.tokenizer.save_pretrained(save_path)
        print(f"âœ… Ù…Ø¯Ù„ Ø¯Ø± `{save_path}` Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

    def load_model(self, load_path=None):
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡.

        :param load_path: Ù…Ø³ÛŒØ± Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡ (Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ØªØ¹ÛŒÛŒÙ†ØŒ Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ `BaseModelConfig` Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
        """
        load_path = load_path or BaseModelConfig.MODEL_SAVE_PATH
        self.model = AutoModelForSequenceClassification.from_pretrained(load_path).to(self.device)
        self.tokenizer = AutoTokenizer.from_pretrained(load_path)
        print(f"âœ… Ù…Ø¯Ù„ Ø§Ø² `{load_path}` Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")


# ==================== ØªØ³Øª ====================
if __name__ == "__main__":
    model = PersianBERTModel(num_labels=3)

    test_text = "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¯Ø± Ø­Ø§Ù„ ØªØºÛŒÛŒØ± Ø¬Ù‡Ø§Ù† Ø§Ø³Øª."
    predictions = model.predict(test_text)

    print("ğŸ“Œ Ù†Ù…Ø±Ø§Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ:", predictions)
    model.save_model()
    model.load_model()
