import torch.optim as optim
from transformers import get_scheduler
from models.base_model.config import BaseModelConfig


class OptimizerManager:
    """
    Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²Ù‡Ø§ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ `learning rate` Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ ParsBERT.
    """

    def __init__(self, model):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ùˆ ØªÙ†Ø¸ÛŒÙ… Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø².

        :param model: Ù…Ø¯Ù„ PyTorch Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´ÙˆØ¯.
        """
        self.model = model
        self.optimizer = self._get_optimizer()
        self.scheduler = self._get_scheduler()

    def _get_optimizer(self):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø².
        """
        optimizer_name = BaseModelConfig.OPTIMIZER.lower()

        if optimizer_name == "adamw":
            return optim.AdamW(self.model.parameters(), lr=BaseModelConfig.LEARNING_RATE, weight_decay=BaseModelConfig.WEIGHT_DECAY)
        elif optimizer_name == "sgd":
            return optim.SGD(self.model.parameters(), lr=BaseModelConfig.LEARNING_RATE, momentum=0.9)
        else:
            raise ValueError(f"âŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² `{optimizer_name}` Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª. Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±: `adamw`, `sgd`")

    def _get_scheduler(self):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ `learning rate scheduler`.
        """
        return get_scheduler(
            name="linear",
            optimizer=self.optimizer,
            num_warmup_steps=BaseModelConfig.WARMUP_STEPS,
            num_training_steps=BaseModelConfig.EPOCHS
        )

    def step(self):
        """
        Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² Ùˆ `scheduler` Ø¯Ø± Ø·ÙˆÙ„ Ø¢Ù…ÙˆØ²Ø´.
        """
        self.optimizer.step()
        self.scheduler.step()

    def zero_grad(self):
        """
        ØªÙ†Ø¸ÛŒÙ… Ù…Ù‚Ø¯Ø§Ø± `gradient` Ø¨Ù‡ ØµÙØ± Ù‚Ø¨Ù„ Ø§Ø² Ù‡Ø± `backpropagation`.
        """
        self.optimizer.zero_grad()

    def get_optimizer(self):
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± `trainer.py`.
        """
        return self.optimizer

    def get_scheduler(self):
        """
        Ø¯Ø±ÛŒØ§ÙØª `scheduler` Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± `trainer.py`.
        """
        return self.scheduler


# ==================== ØªØ³Øª ====================
if __name__ == "__main__":
    from models.base_model.model import PersianBERTModel

    model = PersianBERTModel(num_labels=3).model
    optimizer_manager = OptimizerManager(model)

    print("ğŸ“Œ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡:", optimizer_manager.get_optimizer())
    print("ğŸ“Œ `Scheduler` Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø´Ø¯:", optimizer_manager.get_scheduler())
