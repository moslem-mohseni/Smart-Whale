import json
import torch
from torch.utils.data import Dataset, DataLoader
from models.base_model.config import BaseModelConfig
from preprocessing.tokenizer import PersianTokenizer
from preprocessing.normalizer import PersianNormalizer
from preprocessing.stopwords import PersianStopWords
from preprocessing.spellchecker import PersianSpellChecker


class PersianDataset(Dataset):
    """
    Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ParsBERT.
    """

    def __init__(self, data_path, max_length=512):
        """
        :param data_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ
        :param max_length: Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø¯Ù„
        """
        self.data_path = data_path
        self.max_length = max_length
        self.tokenizer = PersianTokenizer()
        self.normalizer = PersianNormalizer()
        self.stopwords_remover = PersianStopWords()
        self.spell_checker = PersianSpellChecker()

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        self.data = self._load_data()

    def _load_data(self):
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø§Ø² ÙØ§ÛŒÙ„ JSON.
        """
        with open(self.data_path, "r", encoding="utf-8") as file:
            return json.load(file)

    def _preprocess_text(self, text):
        """
        Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†: Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒØŒ Ø­Ø°Ù Ú©Ù„Ù…Ø§Øª ØªÙˆÙ‚ÙØŒ ØªØµØ­ÛŒØ­ Ø§Ù…Ù„Ø§ÛŒÛŒ Ùˆ ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ.
        """
        text = self.normalizer.normalize(text)
        text = self.spell_checker.correct_text(text)
        text = self.stopwords_remover.remove_stopwords(text)
        tokens = self.tokenizer.tokenize(text)

        return tokens[:self.max_length]  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„ ÙˆØ±ÙˆØ¯ÛŒ

    def __len__(self):
        """
        ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡.
        """
        return len(self.data)

    def __getitem__(self, idx):
        """
        Ø¯Ø±ÛŒØ§ÙØª ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§.
        """
        sample = self.data[idx]
        text = sample["text"]
        label = sample.get("label", 0)  # Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ú†Ø³Ø¨

        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†
        tokens = self._preprocess_text(text)

        return {
            "input_ids": tokens,
            "label": torch.tensor(label, dtype=torch.long)
        }


def get_dataloader(data_path, batch_size, shuffle=True):
    """
    Ø§ÛŒØ¬Ø§Ø¯ DataLoader Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± `PyTorch`.
    """
    dataset = PersianDataset(data_path)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4)


# ==================== ØªØ³Øª ====================
if __name__ == "__main__":
    train_loader = get_dataloader(BaseModelConfig.TRAIN_DATA_PATH, BaseModelConfig.BATCH_SIZE)

    for batch in train_loader:
        print("ğŸ“Œ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡:")
        print("ØªÙˆÚ©Ù†â€ŒÙ‡Ø§:", batch["input_ids"][:5])
        print("Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§:", batch["label"][:5])
        break  # ÙÙ‚Ø· ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯
