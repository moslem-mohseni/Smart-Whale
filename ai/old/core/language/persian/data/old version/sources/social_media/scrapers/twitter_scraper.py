import time
import random
import logging
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from infrastructure.redis.service.cache_service import CacheService
from infrastructure.kafka.service.kafka_service import KafkaService
from infrastructure.kafka.service.topic_manager import TopicManager
from infrastructure.clickhouse.service.analytics_service import AnalyticsService

# ØªÙ†Ø¸ÛŒÙ… Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ú©Ø±ÙˆÙ…
CHROME_PROFILE_PATH = r"C:\Users\ASUS\AppData\Local\Google\Chrome\User Data\Profile 7"

# Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ `Redis`ØŒ `Kafka` Ùˆ `ClickHouse`
cache_service = CacheService()
kafka_service = KafkaService()
topic_manager = TopicManager()
analytics_service = AnalyticsService()

# Ø§ÛŒØ¬Ø§Ø¯ `Topic` Ø¯Ø± `Kafka` Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²
topic_manager.create_topic("tweets_topic")

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("twitter_scraper_debug.log", encoding="utf-8"), logging.StreamHandler()]
)


class TwitterScraper:
    def __init__(self, headless=False, max_tweets=50):
        self.max_tweets = max_tweets
        self.tweets = set()
        self.load_cached_tweets()

        # ØªÙ†Ø¸ÛŒÙ… WebDriver
        chrome_options = Options()
        chrome_options.add_argument(f"--user-data-dir={CHROME_PROFILE_PATH}")
        chrome_options.add_argument("--profile-directory=Default")
        if headless:
            chrome_options.add_argument("--headless")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920x1080")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")

        try:
            self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
            logging.info("âœ… WebDriver Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø´Ø¯.")
        except Exception as e:
            logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ WebDriver: {e}")
            self.driver = None

    def load_cached_tweets(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ÛŒ Ú©Ø´ Ø´Ø¯Ù‡ Ø§Ø² `Redis`"""
        cached_tweets = cache_service.get("tweets") or []
        self.tweets = set(cached_tweets)
        logging.info(f"ğŸ”¹ {len(self.tweets)} ØªÙˆÛŒÛŒØª Ø§Ø² `Redis` Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")

    def search_tweets(self, query):
        """Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§"""
        if not self.driver:
            logging.error("âŒ WebDriver Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.")
            return []

        search_url = f"https://x.com/search?q={query}&src=typed_query&f=live"
        logging.info(f"ğŸ” Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† Ø¬Ø³ØªØ¬Ùˆ: {search_url}")
        self.driver.get(search_url)
        time.sleep(5)

        last_height = self.driver.execute_script("return document.body.scrollHeight")
        iteration = 0

        while len(self.tweets) < self.max_tweets:
            iteration += 1
            logging.info(f"ğŸ”„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙØ­Ù‡ {iteration}")

            soup = BeautifulSoup(self.driver.page_source, "html.parser")
            tweet_elements = soup.find_all("div", {"data-testid": "tweetText"})
            logging.info(f"ğŸ” ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡ Ø¯Ø± ØµÙØ­Ù‡ {iteration}: {len(tweet_elements)}")

            for tweet in tweet_elements:
                text = tweet.get_text().strip()
                if text and text not in self.tweets:
                    self.tweets.add(text)
                    cache_service.set("tweets", list(self.tweets))  # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± `Redis`
                    kafka_service.send_message("tweets_topic", text)  # Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ `Kafka`
                    analytics_service.insert_row("tweets", {"text": text, "created_at": "NOW()"})  # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± `ClickHouse`
                    logging.info(f"âœ… ØªÙˆÛŒÛŒØª Ø¬Ø¯ÛŒØ¯ Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯: {text[:50]}...")

                if len(self.tweets) >= self.max_tweets:
                    logging.info("ğŸ¯ ØªØ¹Ø¯Ø§Ø¯ Ù…ÙˆØ±Ø¯Ù†Ø¸Ø± ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯.")
                    break

            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(random.uniform(2, 5))

            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                logging.info("ğŸ”š Ø¨Ù‡ Ø§Ù†ØªÙ‡Ø§ÛŒ ØµÙØ­Ù‡ Ø±Ø³ÛŒØ¯ÛŒÙ….")
                break
            last_height = new_height

        return list(self.tweets)

    def close(self):
        """Ø¨Ø³ØªÙ† WebDriver."""
        if self.driver:
            self.driver.quit()
            logging.info("âœ… WebDriver Ø¨Ø³ØªÙ‡ Ø´Ø¯.")


# =========================
# âœ… **Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±Ù¾Ø±**
# =========================
if __name__ == "__main__":
    scraper = TwitterScraper(headless=False, max_tweets=20)
    tweets = scraper.search_tweets("Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ")

    if tweets:
        logging.info(f"âœ… {len(tweets)} ØªÙˆÛŒÛŒØª Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯.")
    else:
        logging.warning("âš ï¸ Ù‡ÛŒÚ† ØªÙˆÛŒÛŒØªÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯.")

    scraper.close()
